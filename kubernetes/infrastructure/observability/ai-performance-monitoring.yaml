# AI Performance Monitoring for PAKE System
apiVersion: v1
kind: ConfigMap
metadata:
  name: ai-performance-config
  namespace: monitoring
  labels:
    app: prometheus
data:
  ai-metrics-rules.yml: |
    groups:
    - name: pake.ai.performance
      interval: 15s
      rules:
      
      # AI Inference Performance Metrics
      - record: pake:ai_inference_rate_5m
        expr: sum(rate(pake_ai_inference_total[5m])) by (model, instance)
        labels:
          metric_type: throughput
      
      - record: pake:ai_inference_success_rate_5m
        expr: |
          sum(rate(pake_ai_inference_total{status="success"}[5m])) by (model) /
          sum(rate(pake_ai_inference_total[5m])) by (model)
        labels:
          metric_type: reliability
      
      - record: pake:ai_inference_latency_p50
        expr: |
          histogram_quantile(0.50,
            sum(rate(pake_ai_inference_duration_seconds_bucket[5m])) by (le, model)
          )
        labels:
          metric_type: latency
          percentile: "50"
      
      - record: pake:ai_inference_latency_p95
        expr: |
          histogram_quantile(0.95,
            sum(rate(pake_ai_inference_duration_seconds_bucket[5m])) by (le, model)
          )
        labels:
          metric_type: latency
          percentile: "95"
      
      - record: pake:ai_inference_latency_p99
        expr: |
          histogram_quantile(0.99,
            sum(rate(pake_ai_inference_duration_seconds_bucket[5m])) by (le, model)
          )
        labels:
          metric_type: latency
          percentile: "99"
      
      # Token Usage Metrics
      - record: pake:ai_tokens_per_second
        expr: sum(rate(pake_ai_tokens_total[5m])) by (model, token_type)
        labels:
          metric_type: token_usage
      
      - record: pake:ai_tokens_per_request
        expr: |
          sum(rate(pake_ai_tokens_total[5m])) by (model) /
          sum(rate(pake_ai_inference_total[5m])) by (model)
        labels:
          metric_type: efficiency
      
      # Cost Metrics
      - record: pake:ai_cost_per_token
        expr: |
          sum(rate(pake_ai_cost_total[5m])) by (model) /
          sum(rate(pake_ai_tokens_total[5m])) by (model)
        labels:
          metric_type: cost_efficiency
      
      - record: pake:ai_cost_per_request
        expr: |
          sum(rate(pake_ai_cost_total[5m])) by (model) /
          sum(rate(pake_ai_inference_total[5m])) by (model)
        labels:
          metric_type: cost_efficiency
      
      # GPU Utilization Metrics
      - record: pake:gpu_utilization_by_model
        expr: |
          avg(nvidia_gpu_utilization) by (instance, gpu) *
          on(instance) group_left(model) 
          (pake_ai_inference_rate_5m > 0)
        labels:
          metric_type: resource_utilization
      
      - record: pake:gpu_memory_utilization
        expr: |
          (nvidia_gpu_memory_used_bytes / nvidia_gpu_memory_total_bytes) * 100
        labels:
          metric_type: resource_utilization
      
      # Model Loading Performance
      - record: pake:model_load_duration_p95
        expr: |
          histogram_quantile(0.95,
            sum(rate(pake_ai_model_load_duration_seconds_bucket[5m])) by (le, model)
          )
        labels:
          metric_type: model_loading
      
      - record: pake:model_load_success_rate
        expr: |
          sum(rate(pake_ai_model_loads_total{status="success"}[5m])) by (model) /
          sum(rate(pake_ai_model_loads_total[5m])) by (model)
        labels:
          metric_type: model_loading
      
      # Queue Depth and Backpressure
      - record: pake:ai_queue_depth
        expr: sum(pake_ai_queue_size) by (instance, model)
        labels:
          metric_type: queue_management
      
      - record: pake:ai_queue_wait_time_p95
        expr: |
          histogram_quantile(0.95,
            sum(rate(pake_ai_queue_wait_duration_seconds_bucket[5m])) by (le, model)
          )
        labels:
          metric_type: queue_management
      
      # Batch Processing Efficiency
      - record: pake:ai_batch_size_avg
        expr: |
          sum(rate(pake_ai_batch_requests_total[5m])) by (model) /
          sum(rate(pake_ai_batches_total[5m])) by (model)
        labels:
          metric_type: batch_processing
      
      - record: pake:ai_batch_utilization
        expr: |
          (pake:ai_batch_size_avg / on(model) pake_ai_max_batch_size) * 100
        labels:
          metric_type: batch_processing

  ai-alerts.yml: |
    groups:
    - name: pake.ai.alerts
      interval: 30s
      rules:
      
      # AI Performance Alerts
      - alert: AIInferenceLatencyHigh
        expr: pake:ai_inference_latency_p95 > 5.0
        for: 2m
        labels:
          severity: warning
          service: pake-ai
          team: ai-ml
        annotations:
          summary: "High AI inference latency detected"
          description: |
            AI inference P95 latency is {{ $value }}s for model {{ $labels.model }}.
            This exceeds the 5 second threshold and may impact user experience.
            
            Possible causes:
            - Model complexity increased
            - GPU resource constraints
            - Large input sequences
            - Memory fragmentation
          dashboard_url: "https://grafana.pake-system.com/d/ai-performance"
          runbook_url: "https://wiki.pake-system.com/runbooks/ai-latency"
      
      - alert: AIInferenceFailureRateHigh
        expr: (1 - pake:ai_inference_success_rate_5m) > 0.05
        for: 2m
        labels:
          severity: critical
          service: pake-ai
          team: ai-ml
        annotations:
          summary: "High AI inference failure rate"
          description: |
            AI inference failure rate is {{ $value | humanizePercentage }} for model {{ $labels.model }}.
            This exceeds the 5% threshold and indicates service degradation.
      
      - alert: GPUUtilizationLow
        expr: avg(pake:gpu_utilization_by_model) by (model) < 30
        for: 10m
        labels:
          severity: warning
          service: pake-ai
          team: ai-ml
        annotations:
          summary: "Low GPU utilization detected"
          description: |
            GPU utilization is {{ $value }}% for model {{ $labels.model }}.
            This may indicate underutilized resources or batching issues.
      
      - alert: GPUMemoryExhaustion
        expr: pake:gpu_memory_utilization > 95
        for: 2m
        labels:
          severity: critical
          service: pake-ai
          team: ai-ml
        annotations:
          summary: "GPU memory near exhaustion"
          description: |
            GPU memory utilization is {{ $value }}% on {{ $labels.instance }}.
            This may cause out-of-memory errors and inference failures.
      
      - alert: AIQueueBackup
        expr: pake:ai_queue_depth > 100
        for: 5m
        labels:
          severity: warning
          service: pake-ai
          team: ai-ml
        annotations:
          summary: "AI request queue backup detected"
          description: |
            AI request queue depth is {{ $value }} for model {{ $labels.model }}.
            This indicates processing cannot keep up with request rate.
      
      - alert: AITokenCostSpike
        expr: |
          (
            pake:ai_cost_per_token /
            avg_over_time(pake:ai_cost_per_token[1h])
          ) > 2
        for: 5m
        labels:
          severity: warning
          service: pake-ai
          team: ai-ml
        annotations:
          summary: "AI token cost spike detected"
          description: |
            Cost per token has increased by {{ $value }}x for model {{ $labels.model }}.
            This may indicate model change or pricing changes.
      
      - alert: ModelLoadFailures
        expr: (1 - pake:model_load_success_rate) > 0.1
        for: 1m
        labels:
          severity: critical
          service: pake-ai
          team: ai-ml
        annotations:
          summary: "Model loading failures detected"
          description: |
            Model loading failure rate is {{ $value | humanizePercentage }} for {{ $labels.model }}.
            This will prevent new inference requests from being processed.
      
      - alert: BatchUtilizationLow
        expr: pake:ai_batch_utilization < 50
        for: 15m
        labels:
          severity: info
          service: pake-ai
          team: ai-ml
        annotations:
          summary: "Low batch utilization detected"
          description: |
            Batch utilization is {{ $value }}% for model {{ $labels.model }}.
            Consider adjusting batch size or timeout settings for better efficiency.

---
# NVIDIA GPU Exporter for detailed GPU metrics
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nvidia-gpu-exporter
  namespace: monitoring
  labels:
    app: nvidia-gpu-exporter
spec:
  selector:
    matchLabels:
      app: nvidia-gpu-exporter
  template:
    metadata:
      labels:
        app: nvidia-gpu-exporter
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9445"
        prometheus.io/path: "/metrics"
    spec:
      hostPID: true
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      nodeSelector:
        accelerator: nvidia
      containers:
      - name: nvidia-gpu-exporter
        image: nvcr.io/nvidia/k8s/dcgm-exporter:3.1.8-3.1.5-ubuntu20.04
        imagePullPolicy: IfNotPresent
        
        ports:
        - name: metrics
          containerPort: 9445
          protocol: TCP
        
        env:
        - name: DCGM_EXPORTER_LISTEN
          value: ":9445"
        - name: DCGM_EXPORTER_KUBERNETES
          value: "true"
        - name: DCGM_EXPORTER_COLLECTORS
          value: "/etc/dcgm-exporter/dcp-metrics-included.csv"
        
        resources:
          requests:
            cpu: 50m
            memory: 64Mi
          limits:
            cpu: 200m
            memory: 256Mi
        
        volumeMounts:
        - name: proc
          mountPath: /host/proc
          readOnly: true
        - name: sys
          mountPath: /host/sys
          readOnly: true
        
        securityContext:
          privileged: true
          capabilities:
            add: ["SYS_ADMIN"]
      
      volumes:
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
      
      hostNetwork: true
      hostPID: true

---
# AI Model Performance Tracker
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-performance-tracker
  namespace: monitoring
  labels:
    app: ai-performance-tracker
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ai-performance-tracker
  template:
    metadata:
      labels:
        app: ai-performance-tracker
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: ai-performance-tracker
      containers:
      - name: performance-tracker
        image: python:3.11-slim
        imagePullPolicy: IfNotPresent
        
        command:
        - /bin/sh
        - -c
        - |
          pip install prometheus_client requests pandas numpy
          python /app/performance_tracker.py
        
        env:
        - name: PROMETHEUS_URL
          value: "http://prometheus.monitoring.svc.cluster.local:9090"
        - name: AI_SERVICE_URL
          value: "http://pake-ai.pake-system.svc.cluster.local"
        - name: TRACKING_INTERVAL
          value: "60"  # seconds
        
        ports:
        - containerPort: 8080
          name: metrics
        
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 100m
            memory: 256Mi
        
        volumeMounts:
        - name: performance-tracker-code
          mountPath: /app
        
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 30
        
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 10
      
      volumes:
      - name: performance-tracker-code
        configMap:
          name: ai-performance-tracker-code
      
      nodeSelector:
        workload: api-services
        node-type: standard
      tolerations:
      - key: workload
        operator: Equal
        value: api-services
        effect: NoSchedule

---
# AI Performance Tracker Code
apiVersion: v1
kind: ConfigMap
metadata:
  name: ai-performance-tracker-code
  namespace: monitoring
data:
  performance_tracker.py: |
    import os
    import time
    import json
    import requests
    import logging
    from datetime import datetime, timedelta
    from prometheus_client import start_http_server, Counter, Histogram, Gauge, Summary
    import threading
    from http.server import HTTPServer, BaseHTTPRequestHandler
    
    # Configure logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)
    
    # Prometheus metrics
    model_performance_score = Gauge('pake_ai_model_performance_score', 'Overall model performance score', ['model'])
    token_efficiency = Gauge('pake_ai_token_efficiency', 'Tokens per second processing rate', ['model'])
    cost_effectiveness = Gauge('pake_ai_cost_effectiveness', 'Cost effectiveness ratio', ['model'])
    model_accuracy_estimate = Gauge('pake_ai_model_accuracy_estimate', 'Estimated model accuracy', ['model'])
    inference_queue_health = Gauge('pake_ai_queue_health_score', 'Queue health score (0-1)', ['model'])
    gpu_efficiency = Gauge('pake_ai_gpu_efficiency', 'GPU utilization efficiency score', ['gpu_id'])
    
    class AIPerformanceTracker:
        def __init__(self):
            self.prometheus_url = os.getenv('PROMETHEUS_URL', 'http://prometheus:9090')
            self.ai_service_url = os.getenv('AI_SERVICE_URL', 'http://pake-ai:8080')
            self.tracking_interval = int(os.getenv('TRACKING_INTERVAL', '60'))
            
            self.model_baselines = {}
            self.performance_history = {}
        
        def query_prometheus(self, query):
            """Query Prometheus for metrics"""
            try:
                response = requests.get(
                    f"{self.prometheus_url}/api/v1/query",
                    params={'query': query},
                    timeout=10
                )
                response.raise_for_status()
                data = response.json()
                
                if data['status'] == 'success':
                    return data['data']['result']
                else:
                    logger.error(f"Prometheus query failed: {data.get('error')}")
                    return []
            except Exception as e:
                logger.error(f"Error querying Prometheus: {e}")
                return []
        
        def calculate_model_performance_score(self, model):
            """Calculate overall performance score for a model"""
            try:
                # Get metrics
                latency_data = self.query_prometheus(f'pake:ai_inference_latency_p95{{model="{model}"}}')
                success_rate_data = self.query_prometheus(f'pake:ai_inference_success_rate_5m{{model="{model}"}}')
                throughput_data = self.query_prometheus(f'pake:ai_inference_rate_5m{{model="{model}"}}')
                cost_data = self.query_prometheus(f'pake:ai_cost_per_request{{model="{model}"}}')
                
                if not all([latency_data, success_rate_data, throughput_data, cost_data]):
                    logger.warning(f"Incomplete metrics for model {model}")
                    return 0.0
                
                latency = float(latency_data[0]['value'][1])
                success_rate = float(success_rate_data[0]['value'][1])
                throughput = float(throughput_data[0]['value'][1])
                cost = float(cost_data[0]['value'][1])
                
                # Calculate component scores (0-1)
                latency_score = max(0, min(1, (5.0 - latency) / 5.0))  # 5s max acceptable
                reliability_score = success_rate
                throughput_score = min(1, throughput / 10.0)  # 10 req/sec as reference
                cost_score = max(0, min(1, (1.0 - cost) / 1.0))  # $1 per request as reference
                
                # Weighted composite score
                performance_score = (
                    0.3 * latency_score +
                    0.4 * reliability_score +
                    0.2 * throughput_score +
                    0.1 * cost_score
                )
                
                model_performance_score.labels(model=model).set(performance_score)
                logger.info(f"Performance score for {model}: {performance_score:.3f}")
                
                return performance_score
                
            except Exception as e:
                logger.error(f"Error calculating performance score for {model}: {e}")
                return 0.0
        
        def calculate_token_efficiency(self, model):
            """Calculate token processing efficiency"""
            try:
                tokens_per_sec_data = self.query_prometheus(f'pake:ai_tokens_per_second{{model="{model}"}}')
                if not tokens_per_sec_data:
                    return 0.0
                
                tokens_per_sec = sum(float(result['value'][1]) for result in tokens_per_sec_data)
                token_efficiency.labels(model=model).set(tokens_per_sec)
                
                return tokens_per_sec
                
            except Exception as e:
                logger.error(f"Error calculating token efficiency for {model}: {e}")
                return 0.0
        
        def calculate_cost_effectiveness(self, model):
            """Calculate cost effectiveness ratio"""
            try:
                throughput_data = self.query_prometheus(f'pake:ai_inference_rate_5m{{model="{model}"}}')
                cost_data = self.query_prometheus(f'pake:ai_cost_per_request{{model="{model}"}}')
                
                if not all([throughput_data, cost_data]):
                    return 0.0
                
                throughput = float(throughput_data[0]['value'][1])
                cost = float(cost_data[0]['value'][1])
                
                # Cost effectiveness = throughput / cost (higher is better)
                if cost > 0:
                    effectiveness = throughput / cost
                    cost_effectiveness.labels(model=model).set(effectiveness)
                    return effectiveness
                else:
                    return 0.0
                    
            except Exception as e:
                logger.error(f"Error calculating cost effectiveness for {model}: {e}")
                return 0.0
        
        def estimate_model_accuracy(self, model):
            """Estimate model accuracy based on retry patterns and user feedback"""
            try:
                # Look for retry patterns as proxy for accuracy
                retry_data = self.query_prometheus(f'rate(pake_ai_inference_retries_total{{model="{model}"}}[5m])')
                total_requests_data = self.query_prometheus(f'rate(pake_ai_inference_total{{model="{model}"}}[5m])')
                
                if not all([retry_data, total_requests_data]):
                    # Default estimate if no retry data
                    model_accuracy_estimate.labels(model=model).set(0.95)
                    return 0.95
                
                retries = float(retry_data[0]['value'][1])
                total_requests = float(total_requests_data[0]['value'][1])
                
                if total_requests > 0:
                    retry_rate = retries / total_requests
                    # Assume accuracy inversely related to retry rate
                    estimated_accuracy = max(0.5, 1.0 - (retry_rate * 2))
                    model_accuracy_estimate.labels(model=model).set(estimated_accuracy)
                    return estimated_accuracy
                else:
                    model_accuracy_estimate.labels(model=model).set(0.95)
                    return 0.95
                    
            except Exception as e:
                logger.error(f"Error estimating accuracy for {model}: {e}")
                model_accuracy_estimate.labels(model=model).set(0.95)
                return 0.95
        
        def calculate_queue_health(self, model):
            """Calculate queue health score"""
            try:
                queue_depth_data = self.query_prometheus(f'pake:ai_queue_depth{{model="{model}"}}')
                queue_wait_data = self.query_prometheus(f'pake:ai_queue_wait_time_p95{{model="{model}"}}')
                
                if not queue_depth_data:
                    inference_queue_health.labels(model=model).set(1.0)
                    return 1.0
                
                queue_depth = float(queue_depth_data[0]['value'][1])
                queue_wait = float(queue_wait_data[0]['value'][1]) if queue_wait_data else 0
                
                # Health score decreases with queue depth and wait time
                depth_penalty = min(1.0, queue_depth / 100.0)  # 100 as max acceptable
                wait_penalty = min(1.0, queue_wait / 10.0)     # 10s as max acceptable
                
                health_score = max(0.0, 1.0 - (depth_penalty * 0.6) - (wait_penalty * 0.4))
                inference_queue_health.labels(model=model).set(health_score)
                
                return health_score
                
            except Exception as e:
                logger.error(f"Error calculating queue health for {model}: {e}")
                return 1.0
        
        def calculate_gpu_efficiency(self):
            """Calculate GPU utilization efficiency"""
            try:
                gpu_util_data = self.query_prometheus('nvidia_gpu_utilization')
                gpu_memory_data = self.query_prometheus('nvidia_gpu_memory_used_bytes / nvidia_gpu_memory_total_bytes')
                
                if not gpu_util_data:
                    return
                
                for gpu_data in gpu_util_data:
                    gpu_id = gpu_data['metric'].get('gpu', 'unknown')
                    utilization = float(gpu_data['value'][1])
                    
                    # Find corresponding memory utilization
                    memory_util = 0.0
                    for mem_data in gpu_memory_data:
                        if mem_data['metric'].get('gpu') == gpu_id:
                            memory_util = float(mem_data['value'][1]) * 100
                            break
                    
                    # Efficiency combines both compute and memory utilization
                    # Ideal efficiency is when both are high and balanced
                    if utilization > 0 and memory_util > 0:
                        efficiency = min(utilization, memory_util) / 100.0
                    else:
                        efficiency = 0.0
                    
                    gpu_efficiency.labels(gpu_id=gpu_id).set(efficiency)
                    
            except Exception as e:
                logger.error(f"Error calculating GPU efficiency: {e}")
        
        def get_active_models(self):
            """Get list of active AI models"""
            try:
                models_data = self.query_prometheus('group by (model) (pake:ai_inference_rate_5m > 0)')
                return [result['metric']['model'] for result in models_data]
            except Exception as e:
                logger.error(f"Error getting active models: {e}")
                return []
        
        def health_check(self):
            """Health check endpoint"""
            return {"status": "healthy", "last_update": datetime.utcnow().isoformat()}
        
        def run(self):
            """Main tracking loop"""
            logger.info("Starting AI Performance Tracker")
            
            while True:
                try:
                    # Get active models
                    active_models = self.get_active_models()
                    logger.info(f"Tracking performance for models: {active_models}")
                    
                    # Track performance for each model
                    for model in active_models:
                        self.calculate_model_performance_score(model)
                        self.calculate_token_efficiency(model)
                        self.calculate_cost_effectiveness(model)
                        self.estimate_model_accuracy(model)
                        self.calculate_queue_health(model)
                    
                    # Track GPU efficiency
                    self.calculate_gpu_efficiency()
                    
                    logger.info(f"Performance tracking completed for {len(active_models)} models")
                    
                except Exception as e:
                    logger.error(f"Error in performance tracking loop: {e}")
                
                time.sleep(self.tracking_interval)
    
    # Health check server
    tracker = AIPerformanceTracker()
    
    class HealthHandler(BaseHTTPRequestHandler):
        def do_GET(self):
            if self.path == '/health':
                self.send_response(200)
                self.send_header('Content-type', 'application/json')
                self.end_headers()
                self.wfile.write(json.dumps(tracker.health_check()).encode())
            elif self.path == '/ready':
                self.send_response(200)
                self.send_header('Content-type', 'application/json')
                self.end_headers()
                self.wfile.write(json.dumps({"ready": True}).encode())
        
        def log_message(self, format, *args):
            pass  # Suppress logging
    
    # Start health server
    health_server = HTTPServer(('0.0.0.0', 8080), HealthHandler)
    health_thread = threading.Thread(target=health_server.serve_forever)
    health_thread.daemon = True
    health_thread.start()
    
    # Start Prometheus metrics server
    start_http_server(8081)
    
    # Start performance tracking
    tracker.run()

---
# Service Account for AI Performance Tracker
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ai-performance-tracker
  namespace: monitoring

---
# RBAC for AI Performance Tracker
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ai-performance-tracker
  namespace: monitoring
rules:
- apiGroups: [""]
  resources: ["configmaps", "secrets"]
  verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ai-performance-tracker
  namespace: monitoring
subjects:
- kind: ServiceAccount
  name: ai-performance-tracker
  namespace: monitoring
roleRef:
  kind: Role
  name: ai-performance-tracker
  apiGroup: rbac.authorization.k8s.io

---
# Service for AI Performance Tracker
apiVersion: v1
kind: Service
metadata:
  name: ai-performance-tracker
  namespace: monitoring
  labels:
    app: ai-performance-tracker
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8081"
    prometheus.io/path: "/metrics"
spec:
  selector:
    app: ai-performance-tracker
  ports:
  - name: http
    port: 8080
    targetPort: 8080
    protocol: TCP
  - name: metrics
    port: 8081
    targetPort: 8081
    protocol: TCP

---
# ServiceMonitor for AI Performance metrics
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: ai-performance-tracker
  namespace: monitoring
  labels:
    app: ai-performance-tracker
    release: prometheus
spec:
  selector:
    matchLabels:
      app: ai-performance-tracker
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics

---
# ServiceMonitor for GPU metrics
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: nvidia-gpu-exporter
  namespace: monitoring
  labels:
    app: nvidia-gpu-exporter
    release: prometheus
spec:
  selector:
    matchLabels:
      app: nvidia-gpu-exporter
  endpoints:
  - port: metrics
    interval: 15s
    path: /metrics