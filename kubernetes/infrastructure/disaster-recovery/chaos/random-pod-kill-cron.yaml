# Random Pod Kill Chaos Testing for PAKE System
# Non-destructive chaos engineering with automated rollback and monitoring
apiVersion: v1
kind: ConfigMap
metadata:
  name: chaos-pod-kill-config
  namespace: chaos-testing
  labels:
    app: chaos-pod-kill
    chaos-type: pod-kill
data:
  config.yaml: |
    chaos:
      name: "random-pod-kill"
      description: "Randomly kills pods to test resilience and recovery"
      
      # Experiment configuration
      experiment:
        duration: "10m"
        dry_run: false
        safety_checks: true
        auto_rollback: true
        
      # Target configuration
      targets:
        - namespace: "pake-api"
          labels:
            app: "pake-api"
          kill_percentage: 25
          max_pods: 2
          exclude_labels:
            chaos-safe: "false"
            
        - namespace: "pake-ai"
          labels:
            app: "pake-ai-worker"
          kill_percentage: 20
          max_pods: 1
          exclude_labels:
            chaos-safe: "false"
            
        - namespace: "database"
          labels:
            app: "redis"
            role: "replica"
          kill_percentage: 50
          max_pods: 1
          exclude_labels:
            role: "primary"
            chaos-safe: "false"
            
      # Safety constraints
      safety:
        # Never kill these critical pods
        protected_pods:
          - "etcd"
          - "kube-apiserver"
          - "prometheus"
          - "grafana"
          
        # Minimum healthy replicas required
        min_healthy_replicas:
          pake-api: 2
          pake-ai-worker: 1
          redis-replica: 1
          
        # Pre-experiment health checks
        health_checks:
          - name: "api-health"
            url: "http://pake-api.pake-api.svc.cluster.local:8080/health"
            timeout: 10
            
          - name: "ai-health"
            url: "http://pake-ai.pake-ai.svc.cluster.local:8080/health"
            timeout: 10
            
      # Monitoring configuration
      monitoring:
        metrics_interval: 30
        trace_tags:
          experiment: "random-pod-kill"
          environment: "production"
          team: "platform"
          
        success_criteria:
          max_error_rate: 5.0  # 5% error rate threshold
          max_response_time: 2000  # 2s response time threshold
          min_availability: 99.0   # 99% availability threshold

  chaos-pod-kill.py: |
    #!/usr/bin/env python3
    """
    Random Pod Kill Chaos Experiment
    Safely kills random pods to test system resilience
    """

    import asyncio
    import json
    import logging
    import os
    import random
    import time
    import yaml
    from datetime import datetime, timedelta
    from kubernetes import client, config
    from kubernetes.client.rest import ApiException
    import aiohttp
    import requests

    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    logger = logging.getLogger(__name__)

    class PodKillChaosExperiment:
        def __init__(self, config_path):
            with open(config_path, 'r') as f:
                self.config = yaml.safe_load(f)['chaos']
            
            # Initialize Kubernetes client
            try:
                config.load_incluster_config()
            except:
                config.load_kube_config()
            
            self.k8s_client = client.CoreV1Api()
            self.apps_client = client.AppsV1Api()
            
            # Experiment state
            self.experiment_id = f"pod-kill-{int(time.time())}"
            self.start_time = None
            self.killed_pods = []
            self.baseline_metrics = {}
            self.session = None
            
        async def start(self):
            """Initialize the experiment"""
            self.session = aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30))
            logger.info(f"Starting chaos experiment: {self.experiment_id}")
            
        async def stop(self):
            """Cleanup resources"""
            if self.session:
                await self.session.close()
                
        def emit_metric(self, metric_name, value, labels=None):
            """Emit metric to Prometheus pushgateway"""
            try:
                if labels is None:
                    labels = {}
                    
                labels.update({
                    'experiment_id': self.experiment_id,
                    'experiment_type': 'pod-kill',
                    **self.config['monitoring']['trace_tags']
                })
                
                label_str = ','.join([f'{k}="{v}"' for k, v in labels.items()])
                metric_data = f"{metric_name}{{{label_str}}} {value}"
                
                response = requests.post(
                    "http://prometheus-pushgateway.monitoring.svc.cluster.local:9091/metrics/job/chaos-engineering/instance/pod-kill",
                    data=metric_data,
                    headers={'Content-Type': 'text/plain'},
                    timeout=10
                )
                
                if response.status_code != 200:
                    logger.warning(f"Failed to emit metric {metric_name}: HTTP {response.status_code}")
                    
            except Exception as e:
                logger.error(f"Error emitting metric {metric_name}: {e}")
                
        async def perform_health_check(self, check_config):
            """Perform health check on a service"""
            try:
                async with self.session.get(check_config['url']) as response:
                    if response.status == 200:
                        response_time = response.headers.get('X-Response-Time', 0)
                        return True, float(response_time) if response_time else 0
                    else:
                        logger.warning(f"Health check failed for {check_config['name']}: HTTP {response.status}")
                        return False, 0
                        
            except Exception as e:
                logger.error(f"Health check error for {check_config['name']}: {e}")
                return False, 0
                
        async def collect_baseline_metrics(self):
            """Collect baseline metrics before starting chaos"""
            logger.info("Collecting baseline metrics...")
            
            baseline = {
                'timestamp': datetime.utcnow().isoformat(),
                'health_checks': {},
                'pod_counts': {}
            }
            
            # Perform health checks
            for check in self.config['safety']['health_checks']:
                healthy, response_time = await self.perform_health_check(check)
                baseline['health_checks'][check['name']] = {
                    'healthy': healthy,
                    'response_time': response_time
                }
                
                self.emit_metric(
                    'chaos_baseline_health_status',
                    1 if healthy else 0,
                    {'service': check['name']}
                )
                
                if response_time > 0:
                    self.emit_metric(
                        'chaos_baseline_response_time_ms',
                        response_time,
                        {'service': check['name']}
                    )
            
            # Collect pod counts
            for target in self.config['targets']:
                try:
                    pods = self.k8s_client.list_namespaced_pod(
                        namespace=target['namespace'],
                        label_selector=','.join([f"{k}={v}" for k, v in target['labels'].items()])
                    )
                    
                    healthy_pods = sum(1 for pod in pods.items if pod.status.phase == 'Running')
                    baseline['pod_counts'][f"{target['namespace']}-{target['labels']['app']}"] = healthy_pods
                    
                    self.emit_metric(
                        'chaos_baseline_pod_count',
                        healthy_pods,
                        {
                            'namespace': target['namespace'],
                            'app': target['labels']['app']
                        }
                    )
                    
                except Exception as e:
                    logger.error(f"Error collecting pod count for {target['namespace']}: {e}")
            
            self.baseline_metrics = baseline
            logger.info(f"Baseline metrics collected: {baseline}")
            return baseline
            
        def is_pod_protected(self, pod):
            """Check if pod should be protected from chaos"""
            # Check protected pod names
            for protected in self.config['safety']['protected_pods']:
                if protected in pod.metadata.name:
                    return True
                    
            # Check exclude labels
            if pod.metadata.labels:
                for key, value in pod.metadata.labels.items():
                    if key == 'chaos-safe' and value == 'false':
                        return True
                        
            return False
            
        async def validate_safety_constraints(self, target, pods_to_kill):
            """Validate that safety constraints are met"""
            try:
                # Check minimum healthy replicas
                app_name = target['labels']['app']
                current_healthy = len([p for p in self.get_target_pods(target) if p.status.phase == 'Running'])
                min_required = self.config['safety']['min_healthy_replicas'].get(app_name, 1)
                
                if current_healthy - len(pods_to_kill) < min_required:
                    logger.warning(f"Safety constraint violation: {app_name} would have less than {min_required} healthy replicas")
                    return False
                    
                # Validate kill percentage
                max_kills = max(1, int((current_healthy * target['kill_percentage']) / 100))
                max_kills = min(max_kills, target.get('max_pods', 999))
                
                if len(pods_to_kill) > max_kills:
                    logger.warning(f"Safety constraint violation: trying to kill {len(pods_to_kill)} pods, max allowed: {max_kills}")
                    return False
                    
                return True
                
            except Exception as e:
                logger.error(f"Error validating safety constraints: {e}")
                return False
                
        def get_target_pods(self, target):
            """Get pods matching target criteria"""
            try:
                label_selector = ','.join([f"{k}={v}" for k, v in target['labels'].items()])
                
                # Add exclude label selector
                if 'exclude_labels' in target:
                    exclude_selectors = [f"{k}!={v}" for k, v in target['exclude_labels'].items()]
                    if exclude_selectors:
                        label_selector += ',' + ','.join(exclude_selectors)
                
                pods = self.k8s_client.list_namespaced_pod(
                    namespace=target['namespace'],
                    label_selector=label_selector
                )
                
                # Filter out protected pods
                eligible_pods = [pod for pod in pods.items if not self.is_pod_protected(pod)]
                return eligible_pods
                
            except Exception as e:
                logger.error(f"Error getting target pods: {e}")
                return []
                
        async def kill_pod(self, pod, target):
            """Kill a specific pod"""
            try:
                logger.info(f"Killing pod: {pod.metadata.name} in namespace: {pod.metadata.namespace}")
                
                # Record the kill
                kill_record = {
                    'pod_name': pod.metadata.name,
                    'namespace': pod.metadata.namespace,
                    'timestamp': datetime.utcnow().isoformat(),
                    'target_config': target,
                    'uid': pod.metadata.uid
                }
                
                # Delete the pod
                self.k8s_client.delete_namespaced_pod(
                    name=pod.metadata.name,
                    namespace=pod.metadata.namespace,
                    grace_period_seconds=0
                )
                
                self.killed_pods.append(kill_record)
                
                # Emit metrics
                self.emit_metric(
                    'chaos_pods_killed_total',
                    1,
                    {
                        'namespace': pod.metadata.namespace,
                        'app': target['labels']['app'],
                        'pod_name': pod.metadata.name
                    }
                )
                
                logger.info(f"Successfully killed pod: {pod.metadata.name}")
                return True
                
            except ApiException as e:
                logger.error(f"Kubernetes API error killing pod {pod.metadata.name}: {e}")
                self.emit_metric('chaos_pod_kill_failures_total', 1, {'pod_name': pod.metadata.name})
                return False
            except Exception as e:
                logger.error(f"Error killing pod {pod.metadata.name}: {e}")
                self.emit_metric('chaos_pod_kill_failures_total', 1, {'pod_name': pod.metadata.name})
                return False
                
        async def execute_chaos(self):
            """Execute the main chaos experiment"""
            logger.info("Starting pod kill chaos execution...")
            
            total_kills = 0
            
            for target in self.config['targets']:
                try:
                    logger.info(f"Processing target: {target['namespace']}/{target['labels']['app']}")
                    
                    # Get eligible pods
                    eligible_pods = self.get_target_pods(target)
                    
                    if not eligible_pods:
                        logger.warning(f"No eligible pods found for target: {target}")
                        continue
                    
                    # Calculate number of pods to kill
                    kill_count = max(1, int((len(eligible_pods) * target['kill_percentage']) / 100))
                    kill_count = min(kill_count, target.get('max_pods', 999))
                    kill_count = min(kill_count, len(eligible_pods))
                    
                    # Randomly select pods to kill
                    pods_to_kill = random.sample(eligible_pods, kill_count)
                    
                    # Validate safety constraints
                    if not await self.validate_safety_constraints(target, pods_to_kill):
                        logger.warning(f"Skipping target {target} due to safety constraints")
                        continue
                    
                    logger.info(f"Killing {len(pods_to_kill)} pods from {len(eligible_pods)} eligible pods")
                    
                    # Kill selected pods
                    for pod in pods_to_kill:
                        success = await self.kill_pod(pod, target)
                        if success:
                            total_kills += 1
                            
                        # Add delay between kills
                        await asyncio.sleep(2)
                        
                except Exception as e:
                    logger.error(f"Error processing target {target}: {e}")
                    self.emit_metric('chaos_target_failures_total', 1, {'namespace': target['namespace']})
            
            self.emit_metric('chaos_experiment_pods_killed_total', total_kills)
            logger.info(f"Chaos execution completed. Total pods killed: {total_kills}")
            return total_kills
            
        async def monitor_recovery(self):
            """Monitor system recovery after chaos"""
            logger.info("Monitoring system recovery...")
            
            recovery_start = time.time()
            recovery_timeout = 300  # 5 minutes
            
            while time.time() - recovery_start < recovery_timeout:
                try:
                    all_healthy = True
                    
                    # Check health endpoints
                    for check in self.config['safety']['health_checks']:
                        healthy, response_time = await self.perform_health_check(check)
                        
                        self.emit_metric(
                            'chaos_recovery_health_status',
                            1 if healthy else 0,
                            {'service': check['name']}
                        )
                        
                        if not healthy:
                            all_healthy = False
                            
                    # Check pod recovery
                    for target in self.config['targets']:
                        current_pods = self.get_target_pods(target)
                        healthy_pods = sum(1 for pod in current_pods if pod.status.phase == 'Running')
                        
                        app_name = target['labels']['app']
                        min_required = self.config['safety']['min_healthy_replicas'].get(app_name, 1)
                        
                        self.emit_metric(
                            'chaos_recovery_pod_count',
                            healthy_pods,
                            {
                                'namespace': target['namespace'],
                                'app': app_name
                            }
                        )
                        
                        if healthy_pods < min_required:
                            all_healthy = False
                    
                    if all_healthy:
                        recovery_time = time.time() - recovery_start
                        self.emit_metric('chaos_recovery_time_seconds', recovery_time)
                        logger.info(f"System fully recovered in {recovery_time:.2f} seconds")
                        return True
                        
                    await asyncio.sleep(10)
                    
                except Exception as e:
                    logger.error(f"Error during recovery monitoring: {e}")
                    await asyncio.sleep(10)
            
            logger.warning("System did not fully recover within timeout")
            self.emit_metric('chaos_recovery_timeout', 1)
            return False
            
        async def run_experiment(self):
            """Run the complete chaos experiment"""
            try:
                await self.start()
                self.start_time = datetime.utcnow()
                
                logger.info(f"Starting chaos experiment: {self.config['name']}")
                logger.info(f"Description: {self.config['description']}")
                logger.info(f"Duration: {self.config['experiment']['duration']}")
                
                # Emit experiment start metrics
                self.emit_metric('chaos_experiment_start_timestamp', int(time.time()))
                self.emit_metric('chaos_experiment_status', 1, {'status': 'running'})
                
                # Collect baseline metrics
                baseline = await self.collect_baseline_metrics()
                
                # Perform safety checks
                if self.config['experiment']['safety_checks']:
                    all_healthy = all(check['healthy'] for check in baseline['health_checks'].values())
                    if not all_healthy:
                        logger.error("Pre-experiment health checks failed. Aborting experiment.")
                        self.emit_metric('chaos_experiment_status', 0, {'status': 'aborted', 'reason': 'health_check_failed'})
                        return False
                
                # Execute chaos if not dry run
                if not self.config['experiment']['dry_run']:
                    pods_killed = await self.execute_chaos()
                    
                    if pods_killed == 0:
                        logger.warning("No pods were killed. Experiment may be misconfigured.")
                        self.emit_metric('chaos_experiment_status', 0, {'status': 'failed', 'reason': 'no_pods_killed'})
                        return False
                else:
                    logger.info("DRY RUN: Would have executed chaos but dry_run=true")
                    self.emit_metric('chaos_experiment_status', 1, {'status': 'dry_run'})
                
                # Monitor recovery
                if not self.config['experiment']['dry_run']:
                    recovery_success = await self.monitor_recovery()
                    
                    if not recovery_success:
                        logger.error("System failed to recover properly")
                        self.emit_metric('chaos_experiment_status', 0, {'status': 'failed', 'reason': 'recovery_failed'})
                        return False
                
                # Mark experiment as successful
                experiment_duration = (datetime.utcnow() - self.start_time).total_seconds()
                self.emit_metric('chaos_experiment_duration_seconds', experiment_duration)
                self.emit_metric('chaos_experiment_status', 1, {'status': 'completed'})
                
                logger.info(f"Chaos experiment completed successfully in {experiment_duration:.2f} seconds")
                
                # Generate experiment report
                report = {
                    'experiment_id': self.experiment_id,
                    'start_time': self.start_time.isoformat(),
                    'duration_seconds': experiment_duration,
                    'baseline_metrics': baseline,
                    'killed_pods': self.killed_pods,
                    'status': 'success'
                }
                
                logger.info(f"Experiment report: {json.dumps(report, indent=2)}")
                return True
                
            except Exception as e:
                logger.error(f"Chaos experiment failed: {e}")
                self.emit_metric('chaos_experiment_status', 0, {'status': 'failed', 'reason': 'exception'})
                return False
                
            finally:
                await self.stop()

    async def main():
        config_path = os.getenv('CONFIG_PATH', '/etc/config/config.yaml')
        experiment = PodKillChaosExperiment(config_path)
        
        success = await experiment.run_experiment()
        
        if not success:
            exit(1)

    if __name__ == "__main__":
        asyncio.run(main())

---
# Create chaos-testing namespace
apiVersion: v1
kind: Namespace
metadata:
  name: chaos-testing
  labels:
    name: chaos-testing
    chaos-engineering: enabled

---
# Monthly Random Pod Kill CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: chaos-pod-kill-monthly
  namespace: chaos-testing
  labels:
    app: chaos-pod-kill
    schedule: monthly
    chaos-type: pod-kill
spec:
  schedule: '0 10 15 * *' # 15th of each month at 10:00 AM UTC
  timeZone: 'UTC'
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 12 # Keep 1 year of history
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      backoffLimit: 1
      template:
        metadata:
          labels:
            app: chaos-pod-kill
            schedule: monthly
            chaos-type: pod-kill
          annotations:
            chaos.engineering/experiment: 'random-pod-kill'
            chaos.engineering/schedule: 'monthly'
        spec:
          serviceAccountName: chaos-engineering
          restartPolicy: OnFailure
          containers:
            - name: pod-kill-chaos
              image: python:3.11-alpine
              imagePullPolicy: IfNotPresent
              command:
                - /bin/sh
                - -c
                - |
                  pip install --no-cache-dir kubernetes pyyaml aiohttp requests
                  exec python /scripts/chaos-pod-kill.py
              env:
                - name: CONFIG_PATH
                  value: '/etc/config/config.yaml'
                - name: CHAOS_EXPERIMENT_TYPE
                  value: 'pod-kill'
                - name: CHAOS_SCHEDULE
                  value: 'monthly'
              resources:
                requests:
                  cpu: 200m
                  memory: 512Mi
                limits:
                  cpu: 1000m
                  memory: 2Gi
              volumeMounts:
                - name: config
                  mountPath: /etc/config
                - name: scripts
                  mountPath: /scripts
          volumes:
            - name: config
              configMap:
                name: chaos-pod-kill-config
            - name: scripts
              configMap:
                name: chaos-pod-kill-config
                defaultMode: 0755
          nodeSelector:
            workload: chaos-testing

---
# Quarterly Pod Kill (More Aggressive)
apiVersion: batch/v1
kind: CronJob
metadata:
  name: chaos-pod-kill-quarterly
  namespace: chaos-testing
  labels:
    app: chaos-pod-kill
    schedule: quarterly
    chaos-type: pod-kill
spec:
  schedule: '0 14 1 */3 *' # 1st of every 3rd month at 2:00 PM UTC
  timeZone: 'UTC'
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 4
  failedJobsHistoryLimit: 2
  jobTemplate:
    spec:
      backoffLimit: 1
      template:
        metadata:
          labels:
            app: chaos-pod-kill
            schedule: quarterly
            chaos-type: pod-kill
          annotations:
            chaos.engineering/experiment: 'random-pod-kill-aggressive'
            chaos.engineering/schedule: 'quarterly'
        spec:
          serviceAccountName: chaos-engineering
          restartPolicy: OnFailure
          containers:
            - name: pod-kill-chaos
              image: python:3.11-alpine
              imagePullPolicy: IfNotPresent
              command:
                - /bin/sh
                - -c
                - |
                  pip install --no-cache-dir kubernetes pyyaml aiohttp requests

                  # Override config for more aggressive testing
                  cat > /tmp/quarterly-config.yaml << 'EOF'
                  chaos:
                    name: "quarterly-aggressive-pod-kill"
                    description: "Quarterly aggressive pod kill testing"
                    experiment:
                      duration: "15m"
                      dry_run: false
                      safety_checks: true
                      auto_rollback: true
                    targets:
                      - namespace: "pake-api"
                        labels:
                          app: "pake-api"
                        kill_percentage: 50
                        max_pods: 3
                        exclude_labels:
                          chaos-safe: "false"
                      - namespace: "pake-ai"
                        labels:
                          app: "pake-ai-worker"
                        kill_percentage: 40
                        max_pods: 2
                        exclude_labels:
                          chaos-safe: "false"
                      - namespace: "database"
                        labels:
                          app: "redis"
                          role: "replica"
                        kill_percentage: 100
                        max_pods: 2
                        exclude_labels:
                          role: "primary"
                          chaos-safe: "false"
                    safety:
                      protected_pods:
                        - "etcd"
                        - "kube-apiserver"
                        - "prometheus"
                        - "grafana"
                      min_healthy_replicas:
                        pake-api: 1
                        pake-ai-worker: 1
                        redis-replica: 0
                      health_checks:
                        - name: "api-health"
                          url: "http://pake-api.pake-api.svc.cluster.local:8080/health"
                          timeout: 10
                        - name: "ai-health"
                          url: "http://pake-ai.pake-ai.svc.cluster.local:8080/health"
                          timeout: 10
                    monitoring:
                      metrics_interval: 30
                      trace_tags:
                        experiment: "quarterly-aggressive-pod-kill"
                        environment: "production"
                        team: "platform"
                      success_criteria:
                        max_error_rate: 10.0
                        max_response_time: 3000
                        min_availability: 95.0
                  EOF

                  export CONFIG_PATH="/tmp/quarterly-config.yaml"
                  exec python /scripts/chaos-pod-kill.py
              env:
                - name: CHAOS_EXPERIMENT_TYPE
                  value: 'pod-kill-aggressive'
                - name: CHAOS_SCHEDULE
                  value: 'quarterly'
              resources:
                requests:
                  cpu: 200m
                  memory: 512Mi
                limits:
                  cpu: 1000m
                  memory: 2Gi
              volumeMounts:
                - name: config
                  mountPath: /etc/config
                - name: scripts
                  mountPath: /scripts
          volumes:
            - name: config
              configMap:
                name: chaos-pod-kill-config
            - name: scripts
              configMap:
                name: chaos-pod-kill-config
                defaultMode: 0755

---
# ServiceAccount and RBAC for Chaos Engineering
apiVersion: v1
kind: ServiceAccount
metadata:
  name: chaos-engineering
  namespace: chaos-testing
  labels:
    app: chaos-engineering

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: chaos-engineering
  labels:
    app: chaos-engineering
rules:
  # Pod management
  - apiGroups: ['']
    resources: ['pods']
    verbs: ['get', 'list', 'delete', 'watch']
  - apiGroups: ['']
    resources: ['events']
    verbs: ['create', 'patch']
  # Deployment and ReplicaSet access for scaling
  - apiGroups: ['apps']
    resources: ['deployments', 'replicasets', 'statefulsets']
    verbs: ['get', 'list', 'patch', 'update', 'watch']
  # Service and endpoint access for health checks
  - apiGroups: ['']
    resources: ['services', 'endpoints']
    verbs: ['get', 'list', 'watch']
  # ConfigMap and Secret access
  - apiGroups: ['']
    resources: ['configmaps', 'secrets']
    verbs: ['get', 'list']
  # Node access for resource exhaustion tests
  - apiGroups: ['']
    resources: ['nodes']
    verbs: ['get', 'list', 'watch']
  # Network policy access
  - apiGroups: ['networking.k8s.io']
    resources: ['networkpolicies']
    verbs: ['get', 'list', 'create', 'delete', 'patch', 'update']

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: chaos-engineering
  labels:
    app: chaos-engineering
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: chaos-engineering
subjects:
  - kind: ServiceAccount
    name: chaos-engineering
    namespace: chaos-testing
