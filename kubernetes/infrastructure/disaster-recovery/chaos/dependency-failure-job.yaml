# Dependency Failure Chaos Testing for PAKE System
# Tests system behavior when external dependencies fail
apiVersion: v1
kind: ConfigMap
metadata:
  name: chaos-dependency-failure-config
  namespace: chaos-testing
  labels:
    app: chaos-dependency-failure
    chaos-type: dependency-failure
data:
  config.yaml: |
    chaos:
      name: "dependency-failure"
      description: "Dependency failure chaos to test external service resilience"
      
      # Experiment configuration
      experiment:
        duration: "10m"
        dry_run: false
        safety_checks: true
        auto_rollback: true
        
      # Dependency failure scenarios
      scenarios:
        - name: "database-primary-failure"
          description: "Simulate PostgreSQL primary database failure"
          duration: "5m"
          failure_type: "service_unavailable"
          target:
            type: "kubernetes_service"
            namespace: "database"
            service: "pake-postgresql-primary"
            port: 5432
          fallback_verification:
            service: "pake-postgresql-replica-eu"
            expected_promotion: true
            
        - name: "redis-cache-failure"
          description: "Simulate Redis cache complete failure"
          duration: "3m"
          failure_type: "service_unavailable"
          target:
            type: "kubernetes_service"
            namespace: "database"
            service: "redis-master"
            port: 6379
          fallback_verification:
            degraded_mode: true
            cache_bypass_expected: true
            
        - name: "vector-db-failure"
          description: "Simulate ChromaDB vector database failure"
          duration: "4m"
          failure_type: "service_unavailable"
          target:
            type: "kubernetes_service"
            namespace: "database"
            service: "chromadb"
            port: 8000
          fallback_verification:
            fallback_search: true
            degraded_accuracy: true
            
        - name: "external-api-failure"
          description: "Simulate external API dependency failure"
          duration: "6m"
          failure_type: "http_error_injection"
          target:
            type: "external_endpoint"
            endpoints:
              - "https://api.openai.com/v1/chat/completions"
              - "https://api.anthropic.com/v1/messages"
            error_rate: 100
            error_codes: [503, 504, 429]
          fallback_verification:
            local_model_fallback: true
            response_quality_degradation: acceptable
            
        - name: "s3-storage-failure"
          description: "Simulate S3 storage service failure"
          duration: "4m"
          failure_type: "service_unavailable"
          target:
            type: "external_service"
            service: "s3"
            endpoints:
              - "s3.amazonaws.com"
              - "pake-storage-primary.s3.amazonaws.com"
          fallback_verification:
            local_cache_usage: true
            delayed_upload_queue: true
            
      # Safety constraints
      safety:
        # Protected services that should never be failed
        protected_services:
          - "kube-system/kube-dns"
          - "kube-system/kube-apiserver"
          - "monitoring/prometheus"
          - "monitoring/grafana"
          
        # Pre-experiment health checks
        health_checks:
          - name: "api-health"
            url: "http://pake-api.pake-api.svc.cluster.local:8080/health"
            timeout: 10
            
          - name: "ai-health"
            url: "http://pake-ai.pake-ai.svc.cluster.local:8080/health"
            timeout: 10
            
          - name: "database-primary-health"
            url: "http://pake-postgresql-primary.database.svc.cluster.local:5432"
            timeout: 5
            type: "tcp"
            
        # Success criteria during dependency failure
        success_criteria:
          max_error_rate: 25.0       # 25% error rate acceptable during dependency failure
          max_response_time: 10000   # 10s response time during dependency failure
          min_availability: 75.0     # 75% availability during dependency failure
          fallback_activation_time: 30  # 30s max time to activate fallbacks
          
      # Monitoring configuration
      monitoring:
        metrics_interval: 15
        trace_tags:
          experiment: "dependency-failure"
          environment: "production"
          team: "platform"
          
  dependency-failure.py: |
    #!/usr/bin/env python3
    """
    Dependency Failure Chaos Experiment
    Simulates external dependency failures to test system resilience
    """
    
    import asyncio
    import json
    import logging
    import os
    import time
    import yaml
    from datetime import datetime, timedelta
    from kubernetes import client, config
    from kubernetes.client.rest import ApiException
    import aiohttp
    import requests
    
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    logger = logging.getLogger(__name__)
    
    class DependencyFailureChaosExperiment:
        def __init__(self, config_path):
            with open(config_path, 'r') as f:
                self.config = yaml.safe_load(f)['chaos']
            
            # Initialize Kubernetes client
            try:
                config.load_incluster_config()
            except:
                config.load_kube_config()
            
            self.k8s_client = client.CoreV1Api()
            self.apps_client = client.AppsV1Api()
            self.networking_client = client.NetworkingV1Api()
            
            # Experiment state
            self.experiment_id = f"dependency-failure-{int(time.time())}"
            self.start_time = None
            self.active_failures = []
            self.baseline_metrics = {}
            self.session = None
            
        async def start(self):
            """Initialize the experiment"""
            self.session = aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30))
            logger.info(f"Starting dependency failure experiment: {self.experiment_id}")
            
        async def stop(self):
            """Cleanup resources"""
            if self.session:
                await self.session.close()
                
        def emit_metric(self, metric_name, value, labels=None):
            """Emit metric to Prometheus pushgateway"""
            try:
                if labels is None:
                    labels = {}
                    
                labels.update({
                    'experiment_id': self.experiment_id,
                    'experiment_type': 'dependency-failure',
                    **self.config['monitoring']['trace_tags']
                })
                
                label_str = ','.join([f'{k}="{v}"' for k, v in labels.items()])
                metric_data = f"{metric_name}{{{label_str}}} {value}"
                
                response = requests.post(
                    "http://prometheus-pushgateway.monitoring.svc.cluster.local:9091/metrics/job/chaos-engineering/instance/dependency-failure",
                    data=metric_data,
                    headers={'Content-Type': 'text/plain'},
                    timeout=10
                )
                
                if response.status_code != 200:
                    logger.warning(f"Failed to emit metric {metric_name}: HTTP {response.status_code}")
                    
            except Exception as e:
                logger.error(f"Error emitting metric {metric_name}: {e}")
                
        async def perform_health_check(self, check_config):
            """Perform health check on a service"""
            try:
                start_time = time.time()
                
                if check_config.get('type') == 'tcp':
                    # TCP connection check
                    import socket
                    host, port = check_config['url'].replace('http://', '').split(':')
                    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                    sock.settimeout(check_config.get('timeout', 10))
                    result = sock.connect_ex((host, int(port)))
                    sock.close()
                    
                    response_time = (time.time() - start_time) * 1000
                    return result == 0, response_time
                else:
                    # HTTP health check
                    async with self.session.get(check_config['url']) as response:
                        response_time = (time.time() - start_time) * 1000
                        
                        if response.status == 200:
                            return True, response_time
                        else:
                            logger.warning(f"Health check failed for {check_config['name']}: HTTP {response.status}")
                            return False, response_time
                        
            except asyncio.TimeoutError:
                logger.warning(f"Health check timeout for {check_config['name']}")
                return False, check_config.get('timeout', 10) * 1000
            except Exception as e:
                logger.error(f"Health check error for {check_config['name']}: {e}")
                return False, 0
                
        async def collect_baseline_metrics(self):
            """Collect baseline dependency metrics"""
            logger.info("Collecting baseline dependency metrics...")
            
            baseline = {
                'timestamp': datetime.utcnow().isoformat(),
                'health_checks': {},
                'service_endpoints': {},
                'dependency_status': {}
            }
            
            # Perform health checks
            for check in self.config['safety']['health_checks']:
                healthy, response_time = await self.perform_health_check(check)
                baseline['health_checks'][check['name']] = {
                    'healthy': healthy,
                    'response_time': response_time
                }
                
                self.emit_metric(
                    'chaos_baseline_dependency_health_status',
                    1 if healthy else 0,
                    {'service': check['name']}
                )
                
                self.emit_metric(
                    'chaos_baseline_dependency_response_time_ms',
                    response_time,
                    {'service': check['name']}
                )
            
            # Check service endpoints for Kubernetes services
            try:
                for scenario in self.config['scenarios']:
                    target = scenario['target']
                    
                    if target['type'] == 'kubernetes_service':
                        # Get service and endpoint information
                        service = self.k8s_client.read_namespaced_service(
                            name=target['service'],
                            namespace=target['namespace']
                        )
                        
                        endpoints = self.k8s_client.read_namespaced_endpoints(
                            name=target['service'],
                            namespace=target['namespace']
                        )
                        
                        endpoint_count = 0
                        if endpoints.subsets:
                            for subset in endpoints.subsets:
                                if subset.addresses:
                                    endpoint_count += len(subset.addresses)
                        
                        service_key = f"{target['namespace']}/{target['service']}"
                        baseline['service_endpoints'][service_key] = {
                            'cluster_ip': service.spec.cluster_ip,
                            'ports': [port.port for port in service.spec.ports],
                            'endpoint_count': endpoint_count,
                            'type': service.spec.type
                        }
                        
                        self.emit_metric(
                            'chaos_baseline_service_endpoints',
                            endpoint_count,
                            {
                                'namespace': target['namespace'],
                                'service': target['service']
                            }
                        )
                        
            except Exception as e:
                logger.error(f"Error collecting service endpoint metrics: {e}")
            
            self.baseline_metrics = baseline
            logger.info(f"Baseline dependency metrics collected")
            return baseline
            
        def create_service_failure_policy(self, scenario):
            """Create network policy to block access to a Kubernetes service"""
            target = scenario['target']
            policy_name = f"chaos-block-{target['service']}-{self.experiment_id[:8]}"
            
            # Get service to determine selectors
            try:
                service = self.k8s_client.read_namespaced_service(
                    name=target['service'],
                    namespace=target['namespace']
                )
                
                service_selector = service.spec.selector or {}
                
                # Create network policy to block access to service
                network_policy = client.V1NetworkPolicy(
                    api_version="networking.k8s.io/v1",
                    kind="NetworkPolicy",
                    metadata=client.V1ObjectMeta(
                        name=policy_name,
                        namespace=target['namespace'],
                        labels={
                            "chaos-experiment": self.experiment_id,
                            "chaos-type": "dependency-failure",
                            "scenario": scenario['name']
                        }
                    ),
                    spec=client.V1NetworkPolicySpec(
                        pod_selector=client.V1LabelSelector(
                            match_labels=service_selector
                        ),
                        policy_types=["Ingress"],
                        ingress=[]  # Empty list denies all ingress traffic
                    )
                )
                
                return network_policy
                
            except Exception as e:
                logger.error(f"Error creating service failure policy for {target['service']}: {e}")
                return None
                
        def create_fault_injection_pod(self, scenario):
            """Create fault injection pod for external dependencies"""
            target = scenario['target']
            pod_name = f"chaos-fault-injector-{scenario['name']}-{self.experiment_id[:8]}"
            
            # Create fault injection script based on failure type
            if scenario['failure_type'] == 'http_error_injection':
                # Use a proxy to inject HTTP errors
                fault_script = f"""
import time
import threading
from http.server import HTTPServer, BaseHTTPRequestHandler
import json

class FaultInjectionHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        self.send_error(503, "Service Unavailable")
    
    def do_POST(self):
        self.send_error(503, "Service Unavailable")
    
    def do_PUT(self):
        self.send_error(503, "Service Unavailable")

def run_fault_injector():
    server = HTTPServer(('0.0.0.0', 8080), FaultInjectionHandler)
    server.serve_forever()

if __name__ == '__main__':
    print("Starting fault injection proxy...")
    run_fault_injector()
"""
            else:
                # Generic blocking/delay injection
                fault_script = f"""
import time
import signal
import sys

def signal_handler(sig, frame):
    print('Fault injection stopped')
    sys.exit(0)

signal.signal(signal.SIGTERM, signal_handler)

print("Fault injection active - blocking dependency access")
duration = {self._parse_duration(scenario['duration'])}
time.sleep(duration)
print("Fault injection completed")
"""
            
            fault_pod = client.V1Pod(
                api_version="v1",
                kind="Pod",
                metadata=client.V1ObjectMeta(
                    name=pod_name,
                    namespace="chaos-testing",
                    labels={
                        "chaos-experiment": self.experiment_id,
                        "chaos-type": "dependency-failure",
                        "scenario": scenario['name']
                    }
                ),
                spec=client.V1PodSpec(
                    restart_policy="Never",
                    containers=[
                        client.V1Container(
                            name="fault-injector",
                            image="python:3.11-alpine",
                            command=["/bin/sh"],
                            args=["-c", f"echo '{fault_script}' > /tmp/fault.py && python /tmp/fault.py"],
                            ports=[
                                client.V1ContainerPort(
                                    container_port=8080,
                                    name="proxy"
                                )
                            ] if scenario['failure_type'] == 'http_error_injection' else None,
                            resources=client.V1ResourceRequirements(
                                requests={
                                    "cpu": "100m",
                                    "memory": "128Mi"
                                },
                                limits={
                                    "cpu": "500m",
                                    "memory": "512Mi"
                                }
                            )
                        )
                    ]
                )
            )
            
            return fault_pod
            
        async def create_dependency_failure(self, scenario):
            """Create dependency failure for a scenario"""
            try:
                logger.info(f"Creating dependency failure for scenario: {scenario['name']}")
                
                failure_record = {
                    'scenario': scenario['name'],
                    'failure_type': scenario['failure_type'],
                    'target': scenario['target'],
                    'start_time': datetime.utcnow().isoformat(),
                    'duration': scenario['duration'],
                    'artifacts_created': []
                }
                
                if scenario['target']['type'] == 'kubernetes_service':
                    # Create network policy to block service access
                    policy = self.create_service_failure_policy(scenario)
                    if policy:
                        created_policy = self.networking_client.create_namespaced_network_policy(
                            namespace=scenario['target']['namespace'],
                            body=policy
                        )
                        
                        failure_record['artifacts_created'].append({
                            'type': 'network_policy',
                            'name': created_policy.metadata.name,
                            'namespace': created_policy.metadata.namespace
                        })
                        
                        logger.info(f"Created network policy to block service: {scenario['target']['service']}")
                
                elif scenario['target']['type'] in ['external_endpoint', 'external_service']:
                    # Create fault injection pod
                    fault_pod = self.create_fault_injection_pod(scenario)
                    if fault_pod:
                        created_pod = self.k8s_client.create_namespaced_pod(
                            namespace="chaos-testing",
                            body=fault_pod
                        )
                        
                        failure_record['artifacts_created'].append({
                            'type': 'fault_injection_pod',
                            'name': created_pod.metadata.name,
                            'namespace': created_pod.metadata.namespace
                        })
                        
                        logger.info(f"Created fault injection pod: {created_pod.metadata.name}")
                
                self.active_failures.append(failure_record)
                
                # Emit metrics
                self.emit_metric(
                    'chaos_dependency_failures_created_total',
                    1,
                    {
                        'scenario': scenario['name'],
                        'failure_type': scenario['failure_type'],
                        'target_type': scenario['target']['type']
                    }
                )
                
                return failure_record
                
            except ApiException as e:
                logger.error(f"Kubernetes API error creating failure {scenario['name']}: {e}")
                self.emit_metric('chaos_dependency_failure_creation_failures_total', 1, {'scenario': scenario['name']})
                return None
            except Exception as e:
                logger.error(f"Error creating failure {scenario['name']}: {e}")
                self.emit_metric('chaos_dependency_failure_creation_failures_total', 1, {'scenario': scenario['name']})
                return None
                
        async def verify_fallback_behavior(self, scenario, failure_record):
            """Verify that fallback mechanisms are working"""
            try:
                logger.info(f"Verifying fallback behavior for scenario: {scenario['name']}")
                
                fallback_config = scenario.get('fallback_verification', {})
                verification_results = {
                    'scenario': scenario['name'],
                    'timestamp': datetime.utcnow().isoformat(),
                    'checks': {}
                }
                
                # Wait for fallback activation
                fallback_activation_time = self.config['safety']['success_criteria'].get('fallback_activation_time', 30)
                logger.info(f"Waiting {fallback_activation_time}s for fallback activation...")
                await asyncio.sleep(fallback_activation_time)
                
                # Verify specific fallback behaviors
                if fallback_config.get('expected_promotion'):
                    # Check if replica was promoted to primary
                    try:
                        service = fallback_config.get('service')
                        if service:
                            # Check if service endpoints changed (simplified check)
                            endpoints = self.k8s_client.read_namespaced_endpoints(
                                name=service.split('/')[-1],
                                namespace=service.split('/')[0] if '/' in service else 'database'
                            )
                            
                            endpoint_count = 0
                            if endpoints.subsets:
                                for subset in endpoints.subsets:
                                    if subset.addresses:
                                        endpoint_count += len(subset.addresses)
                            
                            verification_results['checks']['replica_promotion'] = {
                                'expected': True,
                                'verified': endpoint_count > 0,
                                'endpoint_count': endpoint_count
                            }
                            
                    except Exception as e:
                        logger.error(f"Error verifying replica promotion: {e}")
                        verification_results['checks']['replica_promotion'] = {
                            'expected': True,
                            'verified': False,
                            'error': str(e)
                        }
                
                if fallback_config.get('degraded_mode'):
                    # Verify service is running in degraded mode
                    degraded_mode_checks = []
                    
                    for check in self.config['safety']['health_checks']:
                        healthy, response_time = await self.perform_health_check(check)
                        degraded_mode_checks.append({
                            'service': check['name'],
                            'healthy': healthy,
                            'response_time': response_time
                        })
                    
                    verification_results['checks']['degraded_mode'] = degraded_mode_checks
                
                if fallback_config.get('cache_bypass_expected'):
                    # Check if cache bypass is working (would need specific metrics)
                    verification_results['checks']['cache_bypass'] = {
                        'expected': True,
                        'note': 'Cache bypass verification requires application-specific metrics'
                    }
                
                if fallback_config.get('local_model_fallback'):
                    # Check if local model fallback is activated
                    verification_results['checks']['local_model_fallback'] = {
                        'expected': True,
                        'note': 'Local model fallback verification requires AI service metrics'
                    }
                
                # Emit fallback verification metrics
                for check_name, check_result in verification_results['checks'].items():
                    verified = check_result.get('verified', True)  # Default to True for notes
                    self.emit_metric(
                        'chaos_fallback_verification_status',
                        1 if verified else 0,
                        {
                            'scenario': scenario['name'],
                            'check_type': check_name
                        }
                    )
                
                logger.info(f"Fallback verification completed for {scenario['name']}: {verification_results}")
                return verification_results
                
            except Exception as e:
                logger.error(f"Error verifying fallback behavior for {scenario['name']}: {e}")
                return {
                    'scenario': scenario['name'],
                    'error': str(e),
                    'timestamp': datetime.utcnow().isoformat()
                }
                
        async def monitor_dependency_impact(self, duration_seconds):
            """Monitor system behavior during dependency failures"""
            logger.info(f"Monitoring dependency failure impact for {duration_seconds} seconds...")
            
            start_time = time.time()
            
            while time.time() - start_time < duration_seconds:
                try:
                    # Check health endpoints
                    for check in self.config['safety']['health_checks']:
                        healthy, response_time = await self.perform_health_check(check)
                        
                        self.emit_metric(
                            'chaos_dependency_failure_health_status',
                            1 if healthy else 0,
                            {'service': check['name']}
                        )
                        
                        self.emit_metric(
                            'chaos_dependency_failure_response_time_ms',
                            response_time,
                            {'service': check['name']}
                        )
                        
                        # Check against success criteria
                        max_response_time = self.config['safety']['success_criteria']['max_response_time']
                        if response_time > max_response_time:
                            logger.warning(f"Response time {response_time}ms exceeds threshold {max_response_time}ms for {check['name']}")
                    
                    # Wait before next check
                    await asyncio.sleep(self.config['monitoring']['metrics_interval'])
                    
                except Exception as e:
                    logger.error(f"Error during dependency failure monitoring: {e}")
                    await asyncio.sleep(10)
            
            logger.info("Dependency failure impact monitoring completed")
            
        async def cleanup_dependency_failures(self):
            """Remove all dependency failure artifacts"""
            logger.info("Cleaning up dependency failure artifacts...")
            
            cleanup_success = True
            
            for failure_record in self.active_failures:
                for artifact in failure_record['artifacts_created']:
                    try:
                        if artifact['type'] == 'network_policy':
                            self.networking_client.delete_namespaced_network_policy(
                                name=artifact['name'],
                                namespace=artifact['namespace']
                            )
                            logger.info(f"Deleted network policy: {artifact['name']}")
                            
                        elif artifact['type'] == 'fault_injection_pod':
                            self.k8s_client.delete_namespaced_pod(
                                name=artifact['name'],
                                namespace=artifact['namespace'],
                                grace_period_seconds=0
                            )
                            logger.info(f"Deleted fault injection pod: {artifact['name']}")
                            
                    except ApiException as e:
                        if e.status == 404:
                            logger.info(f"Artifact {artifact['name']} already deleted")
                        else:
                            logger.error(f"Error deleting artifact {artifact['name']}: {e}")
                            cleanup_success = False
                    except Exception as e:
                        logger.error(f"Error deleting artifact {artifact['name']}: {e}")
                        cleanup_success = False
                
                failure_record['end_time'] = datetime.utcnow().isoformat()
                failure_record['status'] = 'cleaned_up'
            
            self.emit_metric('chaos_dependency_failure_cleanup_status', 1 if cleanup_success else 0)
            return cleanup_success
            
        def _parse_duration(self, duration_str):
            """Parse duration string like '5m' to seconds"""
            if duration_str.endswith('s'):
                return int(duration_str[:-1])
            elif duration_str.endswith('m'):
                return int(duration_str[:-1]) * 60
            elif duration_str.endswith('h'):
                return int(duration_str[:-1]) * 3600
            else:
                return int(duration_str)  # Assume seconds
                
        async def run_experiment(self):
            """Run the complete dependency failure experiment"""
            try:
                await self.start()
                self.start_time = datetime.utcnow()
                
                logger.info(f"Starting dependency failure experiment: {self.config['name']}")
                logger.info(f"Description: {self.config['description']}")
                
                # Emit experiment start metrics
                self.emit_metric('chaos_experiment_start_timestamp', int(time.time()))
                self.emit_metric('chaos_experiment_status', 1, {'status': 'running'})
                
                # Collect baseline metrics
                baseline = await self.collect_baseline_metrics()
                
                # Perform safety checks
                if self.config['experiment']['safety_checks']:
                    all_healthy = all(check['healthy'] for check in baseline['health_checks'].values())
                    if not all_healthy:
                        logger.error("Pre-experiment health checks failed. Aborting experiment.")
                        self.emit_metric('chaos_experiment_status', 0, {'status': 'aborted', 'reason': 'health_check_failed'})
                        return False
                
                # Execute dependency failures if not dry run
                if not self.config['experiment']['dry_run']:
                    failures_created = 0
                    fallback_verifications = []
                    
                    for scenario in self.config['scenarios']:
                        failure = await self.create_dependency_failure(scenario)
                        if failure:
                            failures_created += 1
                            
                            # Verify fallback behavior after brief delay
                            await asyncio.sleep(10)
                            verification = await self.verify_fallback_behavior(scenario, failure)
                            fallback_verifications.append(verification)
                        
                        # Brief delay between failures
                        await asyncio.sleep(5)
                    
                    if failures_created == 0:
                        logger.warning("No dependency failures were created. Experiment may be misconfigured.")
                        self.emit_metric('chaos_experiment_status', 0, {'status': 'failed', 'reason': 'no_failures_created'})
                        return False
                    
                    # Monitor impact during failures
                    max_duration = max(
                        self._parse_duration(scenario.get('duration', '5m')) 
                        for scenario in self.config['scenarios']
                    )
                    
                    await self.monitor_dependency_impact(max_duration)
                    
                    # Cleanup failures (auto-rollback)
                    cleanup_success = await self.cleanup_dependency_failures()
                    
                    if not cleanup_success:
                        logger.error("Failed to cleanup all dependency failures")
                        self.emit_metric('chaos_experiment_status', 0, {'status': 'failed', 'reason': 'cleanup_failed'})
                        return False
                        
                else:
                    logger.info("DRY RUN: Would have created dependency failures but dry_run=true")
                    self.emit_metric('chaos_experiment_status', 1, {'status': 'dry_run'})
                
                # Wait for system to stabilize
                logger.info("Waiting for system stabilization...")
                await asyncio.sleep(60)
                
                # Verify system recovery
                post_experiment_healthy = True
                for check in self.config['safety']['health_checks']:
                    healthy, response_time = await self.perform_health_check(check)
                    if not healthy and check.get('type') != 'tcp':  # Skip TCP checks for failed services
                        post_experiment_healthy = False
                        logger.warning(f"Service {check['name']} not healthy after experiment")
                
                if not post_experiment_healthy:
                    logger.error("System not fully healthy after experiment")
                    self.emit_metric('chaos_experiment_status', 0, {'status': 'failed', 'reason': 'post_experiment_unhealthy'})
                    return False
                
                # Mark experiment as successful
                experiment_duration = (datetime.utcnow() - self.start_time).total_seconds()
                self.emit_metric('chaos_experiment_duration_seconds', experiment_duration)
                self.emit_metric('chaos_experiment_status', 1, {'status': 'completed'})
                
                logger.info(f"Dependency failure experiment completed successfully in {experiment_duration:.2f} seconds")
                
                # Generate experiment report
                report = {
                    'experiment_id': self.experiment_id,
                    'start_time': self.start_time.isoformat(),
                    'duration_seconds': experiment_duration,
                    'baseline_metrics': baseline,
                    'failures_created': self.active_failures,
                    'fallback_verifications': fallback_verifications if not self.config['experiment']['dry_run'] else [],
                    'status': 'success'
                }
                
                logger.info(f"Experiment report: {json.dumps(report, indent=2)}")
                return True
                
            except Exception as e:
                logger.error(f"Dependency failure experiment failed: {e}")
                self.emit_metric('chaos_experiment_status', 0, {'status': 'failed', 'reason': 'exception'})
                
                # Attempt cleanup on failure
                try:
                    await self.cleanup_dependency_failures()
                except:
                    pass
                    
                return False
                
            finally:
                await self.stop()
    
    async def main():
        config_path = os.getenv('CONFIG_PATH', '/etc/config/config.yaml')
        experiment = DependencyFailureChaosExperiment(config_path)
        
        success = await experiment.run_experiment()
        
        if not success:
            exit(1)
    
    if __name__ == "__main__":
        asyncio.run(main())

---
# Quarterly Dependency Failure Job
apiVersion: batch/v1
kind: Job
metadata:
  name: chaos-dependency-failure-quarterly
  namespace: chaos-testing
  labels:
    app: chaos-dependency-failure
    schedule: quarterly
    chaos-type: dependency-failure
spec:
  backoffLimit: 1
  template:
    metadata:
      labels:
        app: chaos-dependency-failure
        schedule: quarterly
        chaos-type: dependency-failure
      annotations:
        chaos.engineering/experiment: "dependency-failure"
        chaos.engineering/schedule: "quarterly"
    spec:
      serviceAccountName: chaos-engineering
      restartPolicy: OnFailure
      containers:
      - name: dependency-failure-chaos
        image: python:3.11-alpine
        imagePullPolicy: IfNotPresent
        command:
        - /bin/sh
        - -c
        - |
          pip install --no-cache-dir kubernetes pyyaml aiohttp requests
          exec python /scripts/dependency-failure.py
        env:
        - name: CONFIG_PATH
          value: "/etc/config/config.yaml"
        - name: CHAOS_EXPERIMENT_TYPE
          value: "dependency-failure"
        - name: CHAOS_SCHEDULE
          value: "quarterly"
        resources:
          requests:
            cpu: 200m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 2Gi
        volumeMounts:
        - name: config
          mountPath: /etc/config
        - name: scripts
          mountPath: /scripts
      volumes:
      - name: config
        configMap:
          name: chaos-dependency-failure-config
      - name: scripts
        configMap:
          name: chaos-dependency-failure-config
          defaultMode: 0755
      nodeSelector:
        workload: chaos-testing

---
# Annual Comprehensive Dependency Failure Job
apiVersion: batch/v1
kind: Job
metadata:
  name: chaos-dependency-failure-annual
  namespace: chaos-testing
  labels:
    app: chaos-dependency-failure
    schedule: annual
    chaos-type: dependency-failure
spec:
  backoffLimit: 2
  template:
    metadata:
      labels:
        app: chaos-dependency-failure
        schedule: annual
        chaos-type: dependency-failure
      annotations:
        chaos.engineering/experiment: "comprehensive-dependency-failure"
        chaos.engineering/schedule: "annual"
    spec:
      serviceAccountName: chaos-engineering
      restartPolicy: OnFailure
      containers:
      - name: dependency-failure-chaos
        image: python:3.11-alpine
        imagePullPolicy: IfNotPresent
        command:
        - /bin/sh
        - -c
        - |
          pip install --no-cache-dir kubernetes pyyaml aiohttp requests
          
          # Create comprehensive annual test config
          cat > /tmp/annual-config.yaml << 'EOF'
          chaos:
            name: "comprehensive-dependency-failure"
            description: "Annual comprehensive dependency failure testing"
            experiment:
              duration: "20m"
              dry_run: false
              safety_checks: true
              auto_rollback: true
            scenarios:
              - name: "complete-database-cluster-failure"
                description: "Simulate complete database cluster failure"
                duration: "8m"
                failure_type: "service_unavailable"
                target:
                  type: "kubernetes_service"
                  namespace: "database"
                  service: "pake-postgresql-primary"
                  port: 5432
                fallback_verification:
                  service: "pake-postgresql-replica-eu"
                  expected_promotion: true
              - name: "all-cache-services-failure"
                description: "Simulate all Redis cache services failure"
                duration: "6m"
                failure_type: "service_unavailable"
                target:
                  type: "kubernetes_service"
                  namespace: "database"
                  service: "redis-master"
                  port: 6379
                fallback_verification:
                  degraded_mode: true
                  cache_bypass_expected: true
              - name: "vector-and-search-failure"
                description: "Simulate complete vector DB and search failure"
                duration: "7m"
                failure_type: "service_unavailable"
                target:
                  type: "kubernetes_service"
                  namespace: "database"
                  service: "chromadb"
                  port: 8000
                fallback_verification:
                  fallback_search: true
                  degraded_accuracy: true
              - name: "all-external-ai-apis-failure"
                description: "Simulate all external AI APIs failure"
                duration: "10m"
                failure_type: "http_error_injection"
                target:
                  type: "external_endpoint"
                  endpoints:
                    - "https://api.openai.com/v1/chat/completions"
                    - "https://api.anthropic.com/v1/messages"
                  error_rate: 100
                  error_codes: [503, 504, 429, 500]
                fallback_verification:
                  local_model_fallback: true
                  response_quality_degradation: acceptable
              - name: "complete-storage-infrastructure-failure"
                description: "Simulate complete storage infrastructure failure"
                duration: "8m"
                failure_type: "service_unavailable"
                target:
                  type: "external_service"
                  service: "s3"
                  endpoints:
                    - "s3.amazonaws.com"
                    - "pake-storage-primary.s3.amazonaws.com"
                    - "pake-backups.s3.amazonaws.com"
                fallback_verification:
                  local_cache_usage: true
                  delayed_upload_queue: true
            safety:
              protected_services:
                - "kube-system/kube-dns"
                - "kube-system/kube-apiserver"
                - "monitoring/prometheus"
                - "monitoring/grafana"
              health_checks:
                - name: "api-health"
                  url: "http://pake-api.pake-api.svc.cluster.local:8080/health"
                  timeout: 15
                - name: "ai-health"
                  url: "http://pake-ai.pake-ai.svc.cluster.local:8080/health"
                  timeout: 15
              success_criteria:
                max_error_rate: 40.0
                max_response_time: 20000
                min_availability: 60.0
                fallback_activation_time: 45
            monitoring:
              metrics_interval: 10
              trace_tags:
                experiment: "comprehensive-dependency-failure"
                environment: "production"
                team: "platform"
                schedule: "annual"
          EOF
          
          export CONFIG_PATH="/tmp/annual-config.yaml"
          exec python /scripts/dependency-failure.py
        env:
        - name: CHAOS_EXPERIMENT_TYPE
          value: "dependency-failure-comprehensive"
        - name: CHAOS_SCHEDULE
          value: "annual"
        resources:
          requests:
            cpu: 300m
            memory: 1Gi
          limits:
            cpu: 1500m
            memory: 4Gi
        volumeMounts:
        - name: config
          mountPath: /etc/config
        - name: scripts
          mountPath: /scripts
      volumes:
      - name: config
        configMap:
          name: chaos-dependency-failure-config
      - name: scripts
        configMap:
          name: chaos-dependency-failure-config
          defaultMode: 0755
