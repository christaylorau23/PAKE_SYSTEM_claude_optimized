"""Intelligence Insight Service

Multi-stage insight generation pipeline implementing Stage 4 of the Personal Intelligence Engine
blueprint. Provides:
- Emerging topic detection using Dynamic Topic Modeling (DTM)
- Time series trend detection and correlation analysis
- Community detection in knowledge graphs
- Cross-system synthesis and insight generation
- Automated alerting for significant patterns

Following PAKE System enterprise standards with comprehensive error handling,
async/await patterns, and production-ready performance.
"""

import asyncio
import logging
import uuid
from collections import defaultdict
from dataclasses import dataclass, field
from datetime import UTC, datetime, timedelta
from enum import Enum
from typing import Any

# Graph analysis
import networkx as nx
import numpy as np
import pandas as pd

# Time series analysis
from gensim.corpora import Dictionary

# Topic modeling and NLP
from gensim.models import LdaModel, LdaMulticore
from gensim.models.coherencemodel import CoherenceModel
from networkx.algorithms import community

# Scientific computing and analysis
from scipy import stats

from src.services.caching.redis_cache_service import CacheService
from src.services.database.vector_database_service import VectorDatabaseService

# PAKE System imports
from src.services.knowledge.intelligence_core_service import (
    IntelligenceCoreService,
)
from src.services.nlp.intelligence_nlp_service import IntelligenceNLPService

logger = logging.getLogger(__name__)


class InsightType(Enum):
    """Types of insights generated by the system."""

    EMERGING_TOPIC = "emerging_topic"
    TREND_DETECTION = "trend_detection"
    CORRELATION_DISCOVERY = "correlation_discovery"
    ANOMALY_DETECTION = "anomaly_detection"
    COMMUNITY_DISCOVERY = "community_discovery"
    PREDICTIVE_SIGNAL = "predictive_signal"
    SYNTHESIS_INSIGHT = "synthesis_insight"


class SignificanceLevel(Enum):
    """Significance levels for insights."""

    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


@dataclass(frozen=True)
class TopicEvolution:
    """Topic evolution tracking over time."""

    topic_id: str
    topic_words: list[tuple[str, float]]
    topic_description: str
    time_periods: list[datetime]
    prevalence_scores: list[float]
    coherence_scores: list[float]
    trend_direction: str  # "emerging", "stable", "declining"
    growth_rate: float
    significance: SignificanceLevel


@dataclass(frozen=True)
class CorrelationInsight:
    """Correlation discovery between different metrics."""

    metric1_name: str
    metric2_name: str
    correlation_coefficient: float
    p_value: float
    lag_periods: int
    confidence_interval: tuple[float, float]
    sample_size: int
    correlation_type: str  # "pearson", "spearman", "kendall"
    temporal_pattern: str  # "leading", "lagging", "concurrent"


@dataclass(frozen=True)
class CommunityInsight:
    """Community detection results from knowledge graphs."""

    community_id: str
    community_description: str
    member_entities: list[str]
    community_size: int
    modularity_score: float
    central_entities: list[tuple[str, float]]  # entity, centrality score
    community_topics: list[str]
    growth_pattern: str
    significance: SignificanceLevel


@dataclass(frozen=True)
class SynthesisInsight:
    """High-level synthesis insight combining multiple signals."""

    insight_id: str
    title: str
    description: str
    confidence_score: float
    significance: SignificanceLevel
    supporting_evidence: list[str]
    component_insights: list[str]  # IDs of contributing insights
    actionable_recommendations: list[str]
    time_horizon: str  # "immediate", "short_term", "medium_term", "long_term"
    categories: list[str]
    created_at: datetime


@dataclass(frozen=True)
class InsightAlert:
    """Alert for significant insights requiring attention."""

    alert_id: str
    insight_type: InsightType
    title: str
    message: str
    urgency: SignificanceLevel
    data_summary: dict[str, Any]
    recommended_actions: list[str]
    expires_at: datetime
    created_at: datetime = field(default_factory=lambda: datetime.now(UTC))


class IntelligenceInsightService:
    """Advanced insight generation service implementing the Personal Intelligence Engine blueprint.

    Provides:
    - Dynamic topic modeling for trend detection
    - Cross-correlation analysis between data streams
    - Network community detection for relationship discovery
    - Multi-signal synthesis for high-level insights
    - Automated alerting for significant patterns
    """

    def __init__(
        self,
        intelligence_core: IntelligenceCoreService,
        nlp_service: IntelligenceNLPService,
        vector_db: VectorDatabaseService,
        cache_service: CacheService | None = None,
    ):
        """Initialize the Intelligence Insight Service.

        Args:
            intelligence_core: Core knowledge service
            nlp_service: NLP processing service
            vector_db: Vector database service
            cache_service: Optional cache service
        """
        self.intelligence_core = intelligence_core
        self.nlp_service = nlp_service
        self.vector_db = vector_db
        self.cache_service = cache_service

        # Topic modeling components
        self.topic_models: dict[str, LdaModel] = {}
        self.topic_dictionaries: dict[str, Dictionary] = {}
        self.topic_evolution_history: dict[str, list[TopicEvolution]] = defaultdict(
            list,
        )

        # Time series tracking
        self.metric_time_series: dict[str, pd.DataFrame] = {}
        self.correlation_cache: dict[str, CorrelationInsight] = {}

        # Graph analysis
        self.knowledge_graph: nx.Graph | None = None
        self.community_history: list[CommunityInsight] = []

        # Insight storage
        self.generated_insights: dict[str, SynthesisInsight] = {}
        self.active_alerts: dict[str, InsightAlert] = {}

        # Configuration
        self.config = {
            "topic_modeling": {
                "num_topics": 10,
                "passes": 20,
                "alpha": "auto",
                "coherence_threshold": 0.4,
                "min_topic_prevalence": 0.05,
            },
            "correlation_analysis": {
                "min_correlation": 0.3,
                "significance_level": 0.05,
                "max_lag_periods": 30,
                "min_sample_size": 10,
            },
            "community_detection": {
                "min_community_size": 3,
                "resolution": 1.0,
                "modularity_threshold": 0.3,
            },
            "insight_synthesis": {
                "min_confidence": 0.6,
                "evidence_threshold": 3,
                "alert_cooldown_hours": 24,
            },
        }

        # Performance tracking
        self._stats = {
            "topics_tracked": 0,
            "correlations_discovered": 0,
            "communities_detected": 0,
            "insights_generated": 0,
            "alerts_sent": 0,
            "processing_time_total_ms": 0.0,
        }

    async def initialize(self) -> bool:
        """Initialize the insight service.

        Returns:
            bool: Success status
        """
        try:
            logger.info("Initializing Intelligence Insight Service...")

            # Initialize knowledge graph
            await self._build_knowledge_graph()

            # Initialize topic tracking
            await self._initialize_topic_tracking()

            # Setup periodic analysis tasks
            await self._setup_periodic_analysis()

            logger.info("Intelligence Insight Service initialized successfully")
            return True

        except Exception as e:
            logger.error(f"Failed to initialize Intelligence Insight Service: {e}")
            return False

    async def _build_knowledge_graph(self) -> None:
        """Build networkx graph from Neo4j knowledge graph."""
        try:
            # This would typically query Neo4j and build a NetworkX graph
            # For now, creating a placeholder
            self.knowledge_graph = nx.Graph()

            # In real implementation, would populate from Neo4j:
            # - Nodes: entities from knowledge base
            # - Edges: relationships with weights based on confidence

            logger.info("Knowledge graph constructed for analysis")

        except Exception as e:
            logger.error(f"Error building knowledge graph: {e}")
            raise

    async def _initialize_topic_tracking(self) -> None:
        """Initialize topic modeling for trend detection."""
        try:
            # Load historical documents for baseline topic modeling
            # This would query the intelligence core for document history
            logger.info("Topic tracking initialized")

        except Exception as e:
            logger.error(f"Error initializing topic tracking: {e}")
            raise

    async def _setup_periodic_analysis(self) -> None:
        """Setup periodic analysis tasks."""
        try:
            # In a production system, would setup scheduled tasks
            # For now, just logging the setup
            logger.info("Periodic analysis tasks configured")

        except Exception as e:
            logger.error(f"Error setting up periodic analysis: {e}")
            raise

    async def detect_emerging_topics(
        self,
        documents: list[str],
        time_period: str = "current",
        num_topics: int | None = None,
    ) -> list[TopicEvolution]:
        """Detect emerging topics using Dynamic Topic Modeling.

        Args:
            documents: List of documents to analyze
            time_period: Time period identifier
            num_topics: Number of topics to extract

        Returns:
            List[TopicEvolution]: Detected topic evolutions
        """
        try:
            if not documents:
                logger.warning("No documents provided for topic detection")
                return []

            start_time = datetime.now()
            num_topics = num_topics or self.config["topic_modeling"]["num_topics"]

            # Process documents through NLP
            processed_docs = []
            for doc in documents:
                # Extract meaningful tokens
                analysis = await self.nlp_service.analyze_document(
                    doc,
                    include_topics=False,
                )

                # Extract tokens from entities and key phrases
                tokens = []
                for entity in analysis.entities:
                    tokens.append(entity.text.lower())

                for phrase, _ in analysis.key_phrases:
                    tokens.extend(phrase.lower().split())

                if tokens:
                    processed_docs.append(tokens)

            if len(processed_docs) < 2:
                logger.warning("Insufficient processed documents for topic modeling")
                return []

            # Create dictionary and corpus
            dictionary = Dictionary(processed_docs)
            dictionary.filter_extremes(no_below=2, no_above=0.5, keep_n=2000)
            corpus = [dictionary.doc2bow(doc) for doc in processed_docs]

            # Train LDA model
            lda_model = LdaMulticore(
                corpus=corpus,
                id2word=dictionary,
                num_topics=num_topics,
                random_state=42,
                passes=self.config["topic_modeling"]["passes"],
                alpha=self.config["topic_modeling"]["alpha"],
                per_word_topics=True,
                workers=4,
            )

            # Calculate coherence
            coherence_model = CoherenceModel(
                model=lda_model,
                texts=processed_docs,
                dictionary=dictionary,
                coherence="c_v",
            )
            coherence_score = coherence_model.get_coherence()

            # Extract topic evolutions
            topic_evolutions = []
            for topic_id in range(num_topics):
                topic_words = lda_model.show_topic(topic_id, topn=10)

                # Generate topic description
                top_words = [word for word, _ in topic_words[:3]]
                description = f"Topic about {', '.join(top_words)}"

                # Analyze trend (simplified - in real implementation would compare with
                # historical data)
                trend_direction = (
                    "emerging"  # Would be calculated from historical comparison
                )
                growth_rate = 0.1  # Placeholder

                # Determine significance based on coherence and prevalence
                topic_prevalence = sum(prob for _, prob in topic_words) / len(
                    topic_words,
                )

                if coherence_score > 0.6 and topic_prevalence > 0.1:
                    significance = SignificanceLevel.HIGH
                elif coherence_score > 0.4 and topic_prevalence > 0.05:
                    significance = SignificanceLevel.MEDIUM
                else:
                    significance = SignificanceLevel.LOW

                evolution = TopicEvolution(
                    topic_id=f"topic_{topic_id}_{time_period}",
                    topic_words=topic_words,
                    topic_description=description,
                    time_periods=[datetime.now(UTC)],
                    prevalence_scores=[topic_prevalence],
                    coherence_scores=[coherence_score],
                    trend_direction=trend_direction,
                    growth_rate=growth_rate,
                    significance=significance,
                )

                topic_evolutions.append(evolution)
                self.topic_evolution_history[time_period].append(evolution)

            # Store models for future comparison
            self.topic_models[time_period] = lda_model
            self.topic_dictionaries[time_period] = dictionary

            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            self._stats["topics_tracked"] += len(topic_evolutions)
            self._stats["processing_time_total_ms"] += processing_time

            logger.info(
                f"Detected {len(topic_evolutions)} topics with coherence {
                    coherence_score:.3f}",
            )
            return topic_evolutions

        except Exception as e:
            logger.error(f"Error detecting emerging topics: {e}")
            return []

    async def analyze_correlations(
        self,
        metric_pairs: list[tuple[str, pd.Series]],
        max_lag: int | None = None,
    ) -> list[CorrelationInsight]:
        """Analyze correlations between different time series metrics.

        Args:
            metric_pairs: List of (metric_name, time_series) tuples
            max_lag: Maximum lag periods to analyze

        Returns:
            List[CorrelationInsight]: Discovered correlations
        """
        try:
            if len(metric_pairs) < 2:
                logger.warning("Need at least 2 metrics for correlation analysis")
                return []

            start_time = datetime.now()
            max_lag = max_lag or self.config["correlation_analysis"]["max_lag_periods"]
            min_corr = self.config["correlation_analysis"]["min_correlation"]
            alpha = self.config["correlation_analysis"]["significance_level"]
            min_samples = self.config["correlation_analysis"]["min_sample_size"]

            correlations = []

            # Analyze all pairs
            for i, (name1, series1) in enumerate(metric_pairs):
                for j, (name2, series2) in enumerate(metric_pairs[i + 1 :], i + 1):
                    if len(series1) < min_samples or len(series2) < min_samples:
                        continue

                    # Align series by index (assuming datetime index)
                    aligned = pd.concat([series1, series2], axis=1, join="inner")
                    if aligned.shape[0] < min_samples:
                        continue

                    s1_aligned = aligned.iloc[:, 0].dropna()
                    s2_aligned = aligned.iloc[:, 1].dropna()

                    # Calculate cross-correlation with different lags
                    best_correlation = 0
                    best_lag = 0
                    best_p_value = 1.0
                    best_type = "pearson"

                    for lag in range(-max_lag, max_lag + 1):
                        if lag == 0:
                            x, y = s1_aligned, s2_aligned
                        elif lag > 0:
                            x, y = s1_aligned[:-lag], s2_aligned[lag:]
                        else:
                            x, y = s1_aligned[-lag:], s2_aligned[:lag]

                        if len(x) < min_samples or len(y) < min_samples:
                            continue

                        # Try different correlation methods
                        for corr_type in ["pearson", "spearman"]:
                            if corr_type == "pearson":
                                corr, p_val = stats.pearsonr(x, y)
                            else:
                                corr, p_val = stats.spearmanr(x, y)

                            if (
                                abs(corr) > abs(best_correlation)
                                and abs(corr) >= min_corr
                            ):
                                best_correlation = corr
                                best_lag = lag
                                best_p_value = p_val
                                best_type = corr_type

                    # If significant correlation found
                    if abs(best_correlation) >= min_corr and best_p_value < alpha:
                        # Calculate confidence interval
                        n = len(s1_aligned)
                        z_score = stats.norm.ppf(1 - alpha / 2)
                        se = 1 / np.sqrt(n - 3)
                        z_corr = np.arctanh(best_correlation)
                        z_lower = z_corr - z_score * se
                        z_upper = z_corr + z_score * se
                        ci_lower = np.tanh(z_lower)
                        ci_upper = np.tanh(z_upper)

                        # Determine temporal pattern
                        if best_lag > 0:
                            temporal_pattern = "leading"  # series1 leads series2
                        elif best_lag < 0:
                            temporal_pattern = "lagging"  # series1 lags series2
                        else:
                            temporal_pattern = "concurrent"

                        correlation_insight = CorrelationInsight(
                            metric1_name=name1,
                            metric2_name=name2,
                            correlation_coefficient=best_correlation,
                            p_value=best_p_value,
                            lag_periods=best_lag,
                            confidence_interval=(ci_lower, ci_upper),
                            sample_size=n,
                            correlation_type=best_type,
                            temporal_pattern=temporal_pattern,
                        )

                        correlations.append(correlation_insight)

            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            self._stats["correlations_discovered"] += len(correlations)
            self._stats["processing_time_total_ms"] += processing_time

            logger.info(f"Discovered {len(correlations)} significant correlations")
            return correlations

        except Exception as e:
            logger.error(f"Error analyzing correlations: {e}")
            return []

    async def detect_communities(
        self,
        graph: nx.Graph | None = None,
        algorithm: str = "louvain",
    ) -> list[CommunityInsight]:
        """Detect communities in the knowledge graph.

        Args:
            graph: Optional specific graph to analyze
            algorithm: Community detection algorithm to use

        Returns:
            List[CommunityInsight]: Detected communities
        """
        try:
            graph = graph or self.knowledge_graph
            if (
                not graph
                or len(graph.nodes())
                < self.config["community_detection"]["min_community_size"]
            ):
                logger.warning("Insufficient graph data for community detection")
                return []

            start_time = datetime.now()
            min_size = self.config["community_detection"]["min_community_size"]
            resolution = self.config["community_detection"]["resolution"]

            # Detect communities using specified algorithm
            if algorithm == "louvain":
                communities_iter = community.greedy_modularity_communities(
                    graph,
                    resolution=resolution,
                )
            elif algorithm == "label_propagation":
                communities_iter = community.label_propagation_communities(graph)
            else:
                communities_iter = community.greedy_modularity_communities(graph)

            communities = list(communities_iter)

            # Calculate modularity
            partition = {}
            for i, comm in enumerate(communities):
                for node in comm:
                    partition[node] = i

            modularity_score = community.modularity(graph, communities)

            # Analyze each community
            community_insights = []
            for i, comm in enumerate(communities):
                if len(comm) < min_size:
                    continue

                # Calculate centrality measures
                subgraph = graph.subgraph(comm)
                centrality = nx.degree_centrality(subgraph)
                central_entities = sorted(
                    centrality.items(),
                    key=lambda x: x[1],
                    reverse=True,
                )[:5]

                # Generate community description
                top_entities = [entity for entity, _ in central_entities[:3]]
                description = f"Community around {', '.join(top_entities)}"

                # Determine significance based on size and modularity
                if len(comm) > 10 and modularity_score > 0.5:
                    significance = SignificanceLevel.HIGH
                elif len(comm) > 5 and modularity_score > 0.3:
                    significance = SignificanceLevel.MEDIUM
                else:
                    significance = SignificanceLevel.LOW

                community_insight = CommunityInsight(
                    community_id=f"community_{i}_{datetime.now().strftime('%Y%m%d')}",
                    community_description=description,
                    member_entities=list(comm),
                    community_size=len(comm),
                    modularity_score=modularity_score,
                    central_entities=central_entities,
                    community_topics=[],  # Would be populated from topic analysis
                    growth_pattern="stable",  # Would be calculated from temporal analysis
                    significance=significance,
                )

                community_insights.append(community_insight)

            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            self._stats["communities_detected"] += len(community_insights)
            self._stats["processing_time_total_ms"] += processing_time

            self.community_history.extend(community_insights)

            logger.info(
                f"Detected {len(community_insights)} communities with modularity {
                    modularity_score:.3f}",
            )
            return community_insights

        except Exception as e:
            logger.error(f"Error detecting communities: {e}")
            return []

    async def generate_synthesis_insights(
        self,
        topic_evolutions: list[TopicEvolution],
        correlations: list[CorrelationInsight],
        communities: list[CommunityInsight],
    ) -> list[SynthesisInsight]:
        """Generate high-level synthesis insights from multiple signals.

        Args:
            topic_evolutions: Detected topic evolutions
            correlations: Discovered correlations
            communities: Detected communities

        Returns:
            List[SynthesisInsight]: Generated synthesis insights
        """
        try:
            start_time = datetime.now()
            synthesis_insights = []
            min_confidence = self.config["insight_synthesis"]["min_confidence"]
            min_evidence = self.config["insight_synthesis"]["evidence_threshold"]

            # Insight 1: Cross-domain trend emergence
            emerging_topics = [
                t
                for t in topic_evolutions
                if t.trend_direction == "emerging"
                and t.significance in [SignificanceLevel.HIGH, SignificanceLevel.MEDIUM]
            ]
            high_corr = [
                c for c in correlations if abs(c.correlation_coefficient) > 0.7
            ]

            if len(emerging_topics) >= 2 and len(high_corr) >= 1:
                evidence = []
                evidence.append(f"Detected {len(emerging_topics)} emerging topics")
                evidence.append(
                    f"Found {len(high_corr)} strong correlations between metrics",
                )

                # Find topic intersections
                topic_words_sets = [
                    set(word for word, _ in topic.topic_words[:5])
                    for topic in emerging_topics
                ]
                common_concepts = (
                    set.intersection(*topic_words_sets)
                    if len(topic_words_sets) > 1
                    else set()
                )

                if common_concepts:
                    evidence.append(
                        f"Common concepts across topics: {
                            ', '.join(list(common_concepts)[:3])
                        }",
                    )

                confidence = min(0.9, 0.6 + 0.1 * len(evidence))

                synthesis_insight = SynthesisInsight(
                    insight_id=str(uuid.uuid4()),
                    title="Cross-Domain Trend Emergence Detected",
                    description=f"Multiple emerging topics with strong correlations suggest a significant cross-domain trend developing around {', '.join(list(common_concepts)[:3]) if common_concepts else 'related concepts'}.",
                    confidence_score=confidence,
                    significance=(
                        SignificanceLevel.HIGH
                        if confidence > 0.8
                        else SignificanceLevel.MEDIUM
                    ),
                    supporting_evidence=evidence,
                    component_insights=[t.topic_id for t in emerging_topics]
                    + [
                        f"corr_{c.metric1_name}_{c.metric2_name}" for c in high_corr[:3]
                    ],
                    actionable_recommendations=[
                        "Monitor these topics closely for investment opportunities",
                        "Investigate the underlying drivers of the correlations",
                        "Consider early positioning in related markets",
                    ],
                    time_horizon="short_term",
                    categories=["trend_analysis", "market_intelligence"],
                    created_at=datetime.now(UTC),
                )

                synthesis_insights.append(synthesis_insight)

            # Insight 2: Community-driven information flow
            large_communities = [
                c
                for c in communities
                if c.community_size > 5 and c.significance != SignificanceLevel.LOW
            ]

            if len(large_communities) >= 2:
                evidence = []
                evidence.append(
                    f"Identified {len(large_communities)} significant communities",
                )

                # Analyze community overlap with emerging topics
                community_entities = set()
                for comm in large_communities:
                    community_entities.update(comm.member_entities)

                topic_entities = set()
                for topic in emerging_topics:
                    topic_entities.update(word for word, _ in topic.topic_words[:10])

                overlap = community_entities.intersection(topic_entities)
                if overlap:
                    evidence.append(
                        f"Community entities overlap with emerging topics: {len(overlap)} common elements",
                    )

                confidence = min(
                    0.85,
                    0.5 + 0.1 * len(evidence) + 0.05 * len(large_communities),
                )

                synthesis_insight = SynthesisInsight(
                    insight_id=str(uuid.uuid4()),
                    title="Community-Driven Information Networks",
                    description="Strong community structures are forming around emerging topics, suggesting coordinated information flow and potential influence networks.",
                    confidence_score=confidence,
                    significance=SignificanceLevel.MEDIUM,
                    supporting_evidence=evidence,
                    component_insights=[c.community_id for c in large_communities],
                    actionable_recommendations=[
                        "Map key influencers within detected communities",
                        "Monitor information propagation patterns",
                        "Identify early signals from community central nodes",
                    ],
                    time_horizon="medium_term",
                    categories=["network_analysis", "influence_mapping"],
                    created_at=datetime.now(UTC),
                )

                synthesis_insights.append(synthesis_insight)

            # Insight 3: Predictive correlation patterns
            leading_correlations = [
                c
                for c in correlations
                if c.temporal_pattern == "leading"
                and abs(c.correlation_coefficient) > 0.6
            ]

            if len(leading_correlations) >= 2:
                evidence = []
                evidence.append(
                    f"Found {len(leading_correlations)} leading indicator relationships",
                )

                # Group by leading metrics
                leading_metrics = defaultdict(list)
                for corr in leading_correlations:
                    if corr.lag_periods > 0:
                        leading_metrics[corr.metric1_name].append(
                            (corr.metric2_name, corr.lag_periods),
                        )
                    else:
                        leading_metrics[corr.metric2_name].append(
                            (corr.metric1_name, abs(corr.lag_periods)),
                        )

                strong_predictors = {
                    metric: relationships
                    for metric, relationships in leading_metrics.items()
                    if len(relationships) >= 2
                }

                if strong_predictors:
                    evidence.append(
                        f"Identified {len(strong_predictors)} strong predictive metrics",
                    )

                confidence = min(0.9, 0.7 + 0.05 * len(strong_predictors))

                synthesis_insight = SynthesisInsight(
                    insight_id=str(uuid.uuid4()),
                    title="Predictive Signal Network Identified",
                    description="Strong leading indicator relationships suggest predictive capability for market movements and trend changes.",
                    confidence_score=confidence,
                    significance=(
                        SignificanceLevel.HIGH
                        if len(strong_predictors) > 2
                        else SignificanceLevel.MEDIUM
                    ),
                    supporting_evidence=evidence,
                    component_insights=[
                        f"corr_{c.metric1_name}_{c.metric2_name}"
                        for c in leading_correlations
                    ],
                    actionable_recommendations=[
                        "Develop automated monitoring for leading indicators",
                        "Create predictive models based on identified relationships",
                        "Set up early warning systems for trend reversals",
                    ],
                    time_horizon="immediate",
                    categories=["predictive_analytics", "early_warning"],
                    created_at=datetime.now(UTC),
                )

                synthesis_insights.append(synthesis_insight)

            # Store generated insights
            for insight in synthesis_insights:
                self.generated_insights[insight.insight_id] = insight

            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            self._stats["insights_generated"] += len(synthesis_insights)
            self._stats["processing_time_total_ms"] += processing_time

            logger.info(f"Generated {len(synthesis_insights)} synthesis insights")
            return synthesis_insights

        except Exception as e:
            logger.error(f"Error generating synthesis insights: {e}")
            return []

    async def create_alerts(
        self,
        insights: list[SynthesisInsight],
    ) -> list[InsightAlert]:
        """Create alerts for significant insights.

        Args:
            insights: List of synthesis insights

        Returns:
            List[InsightAlert]: Generated alerts
        """
        try:
            alerts = []
            cooldown_hours = self.config["insight_synthesis"]["alert_cooldown_hours"]

            for insight in insights:
                # Check if similar alert was recently sent
                recent_alerts = [
                    alert
                    for alert in self.active_alerts.values()
                    if (datetime.now(UTC) - alert.created_at).total_seconds()
                    < cooldown_hours * 3600
                    and any(
                        category in alert.message.lower()
                        for category in insight.categories
                    )
                ]

                if recent_alerts:
                    continue  # Skip if similar alert was recently sent

                # Determine urgency based on significance and confidence
                if insight.significance == SignificanceLevel.CRITICAL or (
                    insight.significance == SignificanceLevel.HIGH
                    and insight.confidence_score > 0.8
                ):
                    urgency = SignificanceLevel.CRITICAL
                elif (
                    insight.significance == SignificanceLevel.HIGH
                    or insight.confidence_score > 0.7
                ):
                    urgency = SignificanceLevel.HIGH
                else:
                    urgency = SignificanceLevel.MEDIUM

                # Create alert
                alert = InsightAlert(
                    alert_id=str(uuid.uuid4()),
                    insight_type=InsightType.SYNTHESIS_INSIGHT,
                    title=f"🔍 {insight.title}",
                    message=f"{insight.description}\n\nConfidence: {
                        insight.confidence_score:.1%}\nTime Horizon: {
                        insight.time_horizon
                    }\n\nEvidence:\n"
                    + "\n".join(
                        [
                            f"• {evidence}"
                            for evidence in insight.supporting_evidence[:3]
                        ],
                    ),
                    urgency=urgency,
                    data_summary={
                        "insight_id": insight.insight_id,
                        "confidence": insight.confidence_score,
                        "categories": insight.categories,
                        "component_count": len(insight.component_insights),
                    },
                    recommended_actions=insight.actionable_recommendations,
                    expires_at=datetime.now(UTC) + timedelta(hours=48),
                )

                alerts.append(alert)
                self.active_alerts[alert.alert_id] = alert

            self._stats["alerts_sent"] += len(alerts)

            logger.info(f"Created {len(alerts)} insight alerts")
            return alerts

        except Exception as e:
            logger.error(f"Error creating alerts: {e}")
            return []

    async def run_comprehensive_analysis(
        self,
        documents: list[str] | None = None,
        time_series_data: dict[str, pd.Series] | None = None,
    ) -> dict[str, Any]:
        """Run comprehensive insight analysis across all systems.

        Args:
            documents: Optional documents for topic analysis
            time_series_data: Optional time series data for correlation analysis

        Returns:
            Dict[str, Any]: Comprehensive analysis results
        """
        try:
            start_time = datetime.now()
            results = {
                "analysis_timestamp": start_time.isoformat(),
                "topic_evolutions": [],
                "correlations": [],
                "communities": [],
                "synthesis_insights": [],
                "alerts": [],
                "processing_time_ms": 0,
            }

            # Run analyses in parallel where possible
            tasks = []

            # Topic evolution analysis
            if documents:
                tasks.append(self.detect_emerging_topics(documents))

            # Community detection
            if self.knowledge_graph:
                tasks.append(self.detect_communities())

            # Correlation analysis
            if time_series_data and len(time_series_data) >= 2:
                metric_pairs = list(time_series_data.items())
                tasks.append(self.analyze_correlations(metric_pairs))

            # Execute parallel analyses
            if tasks:
                analysis_results = await asyncio.gather(*tasks, return_exceptions=True)

                # Process results
                if (
                    documents
                    and len(analysis_results) > 0
                    and not isinstance(analysis_results[0], Exception)
                ):
                    results["topic_evolutions"] = analysis_results[0]

                community_idx = 1 if documents else 0
                if (
                    self.knowledge_graph
                    and len(analysis_results) > community_idx
                    and not isinstance(analysis_results[community_idx], Exception)
                ):
                    results["communities"] = analysis_results[community_idx]

                corr_idx = (
                    2
                    if documents and self.knowledge_graph
                    else (1 if documents or self.knowledge_graph else 0)
                )
                if (
                    time_series_data
                    and len(analysis_results) > corr_idx
                    and not isinstance(analysis_results[corr_idx], Exception)
                ):
                    results["correlations"] = analysis_results[corr_idx]

                # Generate synthesis insights
                synthesis_insights = await self.generate_synthesis_insights(
                    results["topic_evolutions"],
                    results["correlations"],
                    results["communities"],
                )
                results["synthesis_insights"] = synthesis_insights

                # Create alerts
                alerts = await self.create_alerts(synthesis_insights)
                results["alerts"] = alerts

            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            results["processing_time_ms"] = processing_time

            logger.info(f"Comprehensive analysis completed in {processing_time:.2f}ms")
            logger.info(
                f"Results: {len(results['topic_evolutions'])} topics, {
                    len(results['correlations'])
                } correlations, {len(results['communities'])} communities, {
                    len(results['synthesis_insights'])
                } insights, {len(results['alerts'])} alerts",
            )

            return results

        except Exception as e:
            logger.error(f"Error in comprehensive analysis: {e}")
            return {
                "error": str(e),
                "analysis_timestamp": datetime.now().isoformat(),
                "processing_time_ms": (datetime.now() - start_time).total_seconds()
                * 1000,
            }

    async def get_service_stats(self) -> dict[str, Any]:
        """Get comprehensive service statistics."""
        return {
            "performance_stats": self._stats,
            "models_loaded": {
                "topic_models": len(self.topic_models),
                "knowledge_graph": self.knowledge_graph is not None,
                "correlation_cache": len(self.correlation_cache),
            },
            "insights_storage": {
                "generated_insights": len(self.generated_insights),
                "active_alerts": len(self.active_alerts),
                "community_history": len(self.community_history),
                "topic_evolution_periods": len(self.topic_evolution_history),
            },
            "configuration": self.config,
        }

    async def health_check(self) -> dict[str, Any]:
        """Comprehensive health check for the insight service."""
        try:
            # Check dependencies
            core_health = await self.intelligence_core.health_check()
            nlp_health = await self.nlp_service.health_check()
            vector_health = await self.vector_db.health_check()

            dependencies_healthy = (
                core_health.get("status") == "healthy"
                and nlp_health.get("status") == "healthy"
                and vector_health.get("status") == "healthy"
            )

            # Test basic functionality
            test_docs = [
                "This is a test document about artificial intelligence and machine learning.",
                "Technology companies are investing heavily in AI research and development.",
            ]
            topic_test = await self.detect_emerging_topics(test_docs)

            return {
                "status": (
                    "healthy"
                    if dependencies_healthy and len(topic_test) > 0
                    else "degraded"
                ),
                "dependencies": {
                    "intelligence_core": core_health.get("status"),
                    "nlp_service": nlp_health.get("status"),
                    "vector_database": vector_health.get("status"),
                },
                "functionality_test": {
                    "topic_detection": len(topic_test) > 0,
                    "knowledge_graph": self.knowledge_graph is not None,
                    "models_loaded": len(self.topic_models) > 0,
                },
                "performance_stats": self._stats,
            }

        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "dependencies": {
                    "intelligence_core": "unknown",
                    "nlp_service": "unknown",
                    "vector_database": "unknown",
                },
            }


# Singleton instance
_insight_service: IntelligenceInsightService | None = None


async def get_intelligence_insight_service(
    intelligence_core: IntelligenceCoreService,
    nlp_service: IntelligenceNLPService,
    vector_db: VectorDatabaseService,
    cache_service: CacheService | None = None,
) -> IntelligenceInsightService:
    """Get or create Intelligence Insight service singleton.

    Args:
        intelligence_core: Core knowledge service
        nlp_service: NLP processing service
        vector_db: Vector database service
        cache_service: Optional cache service

    Returns:
        IntelligenceInsightService: Initialized service instance
    """
    global _insight_service

    if _insight_service is None:
        _insight_service = IntelligenceInsightService(
            intelligence_core=intelligence_core,
            nlp_service=nlp_service,
            vector_db=vector_db,
            cache_service=cache_service,
        )
        await _insight_service.initialize()

    return _insight_service
