# Network Partition Chaos Testing for PAKE System
# Simulates network failures and partitions to test service resilience
apiVersion: v1
kind: ConfigMap
metadata:
  name: chaos-network-partition-config
  namespace: chaos-testing
  labels:
    app: chaos-network-partition
    chaos-type: network-partition
data:
  config.yaml: |
    chaos:
      name: "network-partition"
      description: "Network partition chaos to test inter-service communication resilience"

      # Experiment configuration
      experiment:
        duration: "5m"
        dry_run: false
        safety_checks: true
        auto_rollback: true

      # Network partition scenarios
      scenarios:
        - name: "api-database-partition"
          description: "Partition API services from database"
          duration: "3m"
          partition_type: "bidirectional"
          source:
            namespace: "pake-api"
            selector:
              app: "pake-api"
          target:
            namespace: "database"
            selector:
              app: "postgresql"
              role: "primary"

        - name: "ai-redis-partition"
          description: "Partition AI workers from Redis cache"
          duration: "2m"
          partition_type: "unidirectional"
          direction: "to"  # Block AI -> Redis
          source:
            namespace: "pake-ai"
            selector:
              app: "pake-ai-worker"
          target:
            namespace: "database"
            selector:
              app: "redis"

        - name: "cross-region-partition"
          description: "Simulate cross-region network issues"
          duration: "4m"
          partition_type: "latency_injection"
          latency: "200ms"
          jitter: "50ms"
          loss_rate: "2%"
          source:
            namespace: "pake-api"
            selector:
              app: "pake-api"
          target:
            namespace: "database"
            selector:
              app: "pake-postgresql-replica-eu"

      # Safety constraints
      safety:
        # Protected services that should never be partitioned
        protected_services:
          - "kube-system/kube-dns"
          - "kube-system/kube-apiserver"
          - "monitoring/prometheus"
          - "monitoring/grafana"

        # Pre-experiment health checks
        health_checks:
          - name: "api-health"
            url: "http://pake-api.pake-api.svc.cluster.local:8080/health"
            timeout: 10
            required: true

          - name: "ai-health"
            url: "http://pake-ai.pake-ai.svc.cluster.local:8080/health"
            timeout: 10
            required: true

        # Success criteria
        success_criteria:
          max_error_rate: 15.0  # 15% error rate acceptable during network partition
          max_response_time: 5000  # 5s response time during partition
          min_availability: 85.0   # 85% availability during partition

      # Monitoring configuration
      monitoring:
        metrics_interval: 15
        trace_tags:
          experiment: "network-partition"
          environment: "production"
          team: "platform"

  network-partition.py: |
    #!/usr/bin/env python3
    """
    Network Partition Chaos Experiment
    Creates controlled network partitions to test system resilience
    """

    import asyncio
    import json
    import logging
    import os
    import time
    import yaml
    from datetime import datetime, timedelta
    from kubernetes import client, config
    from kubernetes.client.rest import ApiException
    import aiohttp
    import requests

    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    logger = logging.getLogger(__name__)

    class NetworkPartitionChaosExperiment:
        def __init__(self, config_path):
            with open(config_path, 'r') as f:
                self.config = yaml.safe_load(f)['chaos']

            # Initialize Kubernetes client
            try:
                config.load_incluster_config()
            except:
                config.load_kube_config()

            self.k8s_client = client.CoreV1Api()
            self.networking_client = client.NetworkingV1Api()

            # Experiment state
            self.experiment_id = f"network-partition-{int(time.time())}"
            self.start_time = None
            self.active_partitions = []
            self.baseline_metrics = {}
            self.session = None

        async def start(self):
            """Initialize the experiment"""
            self.session = aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30))
            logger.info(f"Starting network partition experiment: {self.experiment_id}")

        async def stop(self):
            """Cleanup resources"""
            if self.session:
                await self.session.close()

        def emit_metric(self, metric_name, value, labels=None):
            """Emit metric to Prometheus pushgateway"""
            try:
                if labels is None:
                    labels = {}

                labels.update({
                    'experiment_id': self.experiment_id,
                    'experiment_type': 'network-partition',
                    **self.config['monitoring']['trace_tags']
                })

                label_str = ','.join([f'{k}="{v}"' for k, v in labels.items()])
                metric_data = f"{metric_name}{{{label_str}}} {value}"

                response = requests.post(
                    "http://prometheus-pushgateway.monitoring.svc.cluster.local:9091/metrics/job/chaos-engineering/instance/network-partition",
                    data=metric_data,
                    headers={'Content-Type': 'text/plain'},
                    timeout=10
                )

                if response.status_code != 200:
                    logger.warning(f"Failed to emit metric {metric_name}: HTTP {response.status_code}")

            except Exception as e:
                logger.error(f"Error emitting metric {metric_name}: {e}")

        async def perform_health_check(self, check_config):
            """Perform health check on a service"""
            try:
                start_time = time.time()
                async with self.session.get(check_config['url']) as response:
                    response_time = (time.time() - start_time) * 1000  # Convert to ms

                    if response.status == 200:
                        return True, response_time
                    else:
                        logger.warning(f"Health check failed for {check_config['name']}: HTTP {response.status}")
                        return False, response_time

            except asyncio.TimeoutError:
                logger.warning(f"Health check timeout for {check_config['name']}")
                return False, check_config.get('timeout', 10) * 1000
            except Exception as e:
                logger.error(f"Health check error for {check_config['name']}: {e}")
                return False, 0

        async def collect_baseline_metrics(self):
            """Collect baseline metrics before network partition"""
            logger.info("Collecting baseline network metrics...")

            baseline = {
                'timestamp': datetime.utcnow().isoformat(),
                'health_checks': {},
                'network_policies': []
            }

            # Perform health checks
            for check in self.config['safety']['health_checks']:
                healthy, response_time = await self.perform_health_check(check)
                baseline['health_checks'][check['name']] = {
                    'healthy': healthy,
                    'response_time': response_time
                }

                self.emit_metric(
                    'chaos_baseline_network_health_status',
                    1 if healthy else 0,
                    {'service': check['name']}
                )

                self.emit_metric(
                    'chaos_baseline_network_response_time_ms',
                    response_time,
                    {'service': check['name']}
                )

            # List existing network policies
            try:
                for scenario in self.config['scenarios']:
                    source_ns = scenario['source']['namespace']
                    target_ns = scenario['target']['namespace']

                    # Get existing network policies
                    policies = self.networking_client.list_namespaced_network_policy(namespace=source_ns)
                    baseline['network_policies'].extend([
                        {
                            'name': policy.metadata.name,
                            'namespace': policy.metadata.namespace,
                            'uid': policy.metadata.uid
                        }
                        for policy in policies.items
                        if 'chaos' not in policy.metadata.name
                    ])

            except Exception as e:
                logger.error(f"Error listing network policies: {e}")

            self.baseline_metrics = baseline
            logger.info(f"Baseline network metrics collected")
            return baseline

        def create_network_policy_deny_all(self, namespace, scenario_name):
            """Create a network policy that denies all traffic"""
            policy_name = f"chaos-deny-all-{scenario_name}-{self.experiment_id[:8]}"

            network_policy = client.V1NetworkPolicy(
                api_version="networking.k8s.io/v1",
                kind="NetworkPolicy",
                metadata=client.V1ObjectMeta(
                    name=policy_name,
                    namespace=namespace,
                    labels={
                        "chaos-experiment": self.experiment_id,
                        "chaos-type": "network-partition",
                        "scenario": scenario_name
                    }
                ),
                spec=client.V1NetworkPolicySpec(
                    pod_selector=client.V1LabelSelector(
                        match_labels={}  # Empty selector matches all pods
                    ),
                    policy_types=["Ingress", "Egress"],
                    ingress=[],  # Empty list denies all ingress
                    egress=[]    # Empty list denies all egress
                )
            )

            return network_policy

        def create_network_policy_selective_deny(self, scenario):
            """Create a network policy for selective partition"""
            source_ns = scenario['source']['namespace']
            source_selector = scenario['source']['selector']
            target_ns = scenario['target']['namespace']
            target_selector = scenario['target']['selector']

            policy_name = f"chaos-partition-{scenario['name']}-{self.experiment_id[:8]}"

            # Create egress rule to deny traffic to target
            egress_rules = []

            if scenario['partition_type'] in ['bidirectional', 'unidirectional']:
                # Block traffic to target namespace/pods
                egress_rules.append(
                    client.V1NetworkPolicyEgressRule(
                        to=[
                            client.V1NetworkPolicyPeer(
                                namespace_selector=client.V1LabelSelector(
                                    match_labels={"name": target_ns}
                                ),
                                pod_selector=client.V1LabelSelector(
                                    match_labels=target_selector
                                )
                            )
                        ]
                    )
                )

            # Allow all other traffic
            egress_rules.append(
                client.V1NetworkPolicyEgressRule(
                    to=[],  # Empty to allows all destinations not explicitly denied
                    ports=[
                        client.V1NetworkPolicyPort(protocol="TCP"),
                        client.V1NetworkPolicyPort(protocol="UDP")
                    ]
                )
            )

            network_policy = client.V1NetworkPolicy(
                api_version="networking.k8s.io/v1",
                kind="NetworkPolicy",
                metadata=client.V1ObjectMeta(
                    name=policy_name,
                    namespace=source_ns,
                    labels={
                        "chaos-experiment": self.experiment_id,
                        "chaos-type": "network-partition",
                        "scenario": scenario['name']
                    }
                ),
                spec=client.V1NetworkPolicySpec(
                    pod_selector=client.V1LabelSelector(
                        match_labels=source_selector
                    ),
                    policy_types=["Egress"],
                    egress=egress_rules
                )
            )

            return network_policy

        async def create_network_partition(self, scenario):
            """Create network partition for a scenario"""
            try:
                logger.info(f"Creating network partition for scenario: {scenario['name']}")

                partition_record = {
                    'scenario': scenario['name'],
                    'start_time': datetime.utcnow().isoformat(),
                    'policies_created': [],
                    'partition_type': scenario['partition_type']
                }

                if scenario['partition_type'] == 'latency_injection':
                    # For latency injection, we would need a different approach
                    # This would typically require a service mesh like Istio
                    logger.warning("Latency injection requires service mesh - simulating with delay")

                    # Simulate by creating a temporary network policy that causes timeouts
                    policy = self.create_network_policy_selective_deny(scenario)

                elif scenario['partition_type'] in ['bidirectional', 'unidirectional']:
                    # Create selective network policy
                    policy = self.create_network_policy_selective_deny(scenario)

                else:
                    logger.error(f"Unknown partition type: {scenario['partition_type']}")
                    return None

                # Apply the network policy
                created_policy = self.networking_client.create_namespaced_network_policy(
                    namespace=scenario['source']['namespace'],
                    body=policy
                )

                partition_record['policies_created'].append({
                    'name': created_policy.metadata.name,
                    'namespace': created_policy.metadata.namespace,
                    'uid': created_policy.metadata.uid
                })

                # For bidirectional partition, create reverse policy
                if scenario['partition_type'] == 'bidirectional':
                    reverse_scenario = scenario.copy()
                    reverse_scenario['source'], reverse_scenario['target'] = scenario['target'], scenario['source']
                    reverse_policy = self.create_network_policy_selective_deny(reverse_scenario)
                    reverse_policy.metadata.name = f"chaos-partition-reverse-{scenario['name']}-{self.experiment_id[:8]}"

                    created_reverse = self.networking_client.create_namespaced_network_policy(
                        namespace=reverse_scenario['source']['namespace'],
                        body=reverse_policy
                    )

                    partition_record['policies_created'].append({
                        'name': created_reverse.metadata.name,
                        'namespace': created_reverse.metadata.namespace,
                        'uid': created_reverse.metadata.uid
                    })

                self.active_partitions.append(partition_record)

                # Emit metrics
                self.emit_metric(
                    'chaos_network_partitions_created_total',
                    1,
                    {
                        'scenario': scenario['name'],
                        'partition_type': scenario['partition_type'],
                        'source_namespace': scenario['source']['namespace'],
                        'target_namespace': scenario['target']['namespace']
                    }
                )

                logger.info(f"Successfully created network partition: {scenario['name']}")
                return partition_record

            except ApiException as e:
                logger.error(f"Kubernetes API error creating partition {scenario['name']}: {e}")
                self.emit_metric('chaos_network_partition_failures_total', 1, {'scenario': scenario['name']})
                return None
            except Exception as e:
                logger.error(f"Error creating partition {scenario['name']}: {e}")
                self.emit_metric('chaos_network_partition_failures_total', 1, {'scenario': scenario['name']})
                return None

        async def remove_network_partition(self, partition_record):
            """Remove network partition by deleting policies"""
            try:
                logger.info(f"Removing network partition: {partition_record['scenario']}")

                for policy_info in partition_record['policies_created']:
                    try:
                        self.networking_client.delete_namespaced_network_policy(
                            name=policy_info['name'],
                            namespace=policy_info['namespace']
                        )
                        logger.info(f"Deleted network policy: {policy_info['name']}")

                    except ApiException as e:
                        if e.status == 404:
                            logger.warning(f"Network policy {policy_info['name']} not found (already deleted)")
                        else:
                            logger.error(f"Error deleting network policy {policy_info['name']}: {e}")

                # Update partition record
                partition_record['end_time'] = datetime.utcnow().isoformat()
                partition_record['status'] = 'removed'

                # Emit metrics
                duration = (datetime.utcnow() - datetime.fromisoformat(partition_record['start_time'])).total_seconds()
                self.emit_metric(
                    'chaos_network_partition_duration_seconds',
                    duration,
                    {'scenario': partition_record['scenario']}
                )

                return True

            except Exception as e:
                logger.error(f"Error removing partition {partition_record['scenario']}: {e}")
                return False

        async def monitor_partition_impact(self, duration_seconds):
            """Monitor system behavior during network partition"""
            logger.info(f"Monitoring partition impact for {duration_seconds} seconds...")

            start_time = time.time()

            while time.time() - start_time < duration_seconds:
                try:
                    # Check health endpoints
                    for check in self.config['safety']['health_checks']:
                        healthy, response_time = await self.perform_health_check(check)

                        self.emit_metric(
                            'chaos_partition_health_status',
                            1 if healthy else 0,
                            {'service': check['name']}
                        )

                        self.emit_metric(
                            'chaos_partition_response_time_ms',
                            response_time,
                            {'service': check['name']}
                        )

                        # Check against success criteria
                        max_response_time = self.config['safety']['success_criteria']['max_response_time']
                        if response_time > max_response_time:
                            logger.warning(f"Response time {response_time}ms exceeds threshold {max_response_time}ms for {check['name']}")

                    # Wait before next check
                    await asyncio.sleep(self.config['monitoring']['metrics_interval'])

                except Exception as e:
                    logger.error(f"Error during partition monitoring: {e}")
                    await asyncio.sleep(10)

            logger.info("Partition impact monitoring completed")

        async def cleanup_all_partitions(self):
            """Remove all active network partitions"""
            logger.info("Cleaning up all active network partitions...")

            cleanup_success = True

            for partition in self.active_partitions:
                if partition.get('status') != 'removed':
                    success = await self.remove_network_partition(partition)
                    if not success:
                        cleanup_success = False

            # Also clean up any orphaned chaos network policies
            try:
                all_namespaces = self.k8s_client.list_namespace()
                for ns in all_namespaces.items:
                    try:
                        policies = self.networking_client.list_namespaced_network_policy(namespace=ns.metadata.name)
                        for policy in policies.items:
                            if (policy.metadata.labels and
                                policy.metadata.labels.get('chaos-experiment') == self.experiment_id):
                                logger.info(f"Cleaning up orphaned policy: {policy.metadata.name}")
                                self.networking_client.delete_namespaced_network_policy(
                                    name=policy.metadata.name,
                                    namespace=policy.metadata.namespace
                                )
                    except Exception as e:
                        logger.error(f"Error cleaning policies in namespace {ns.metadata.name}: {e}")

            except Exception as e:
                logger.error(f"Error during orphaned policy cleanup: {e}")
                cleanup_success = False

            self.emit_metric('chaos_partition_cleanup_status', 1 if cleanup_success else 0)
            return cleanup_success

        async def run_experiment(self):
            """Run the complete network partition experiment"""
            try:
                await self.start()
                self.start_time = datetime.utcnow()

                logger.info(f"Starting network partition experiment: {self.config['name']}")
                logger.info(f"Description: {self.config['description']}")

                # Emit experiment start metrics
                self.emit_metric('chaos_experiment_start_timestamp', int(time.time()))
                self.emit_metric('chaos_experiment_status', 1, {'status': 'running'})

                # Collect baseline metrics
                baseline = await self.collect_baseline_metrics()

                # Perform safety checks
                if self.config['experiment']['safety_checks']:
                    all_healthy = all(check['healthy'] for check in baseline['health_checks'].values())
                    if not all_healthy:
                        logger.error("Pre-experiment health checks failed. Aborting experiment.")
                        self.emit_metric('chaos_experiment_status', 0, {'status': 'aborted', 'reason': 'health_check_failed'})
                        return False

                # Execute network partitions if not dry run
                if not self.config['experiment']['dry_run']:
                    partitions_created = 0

                    for scenario in self.config['scenarios']:
                        partition = await self.create_network_partition(scenario)
                        if partition:
                            partitions_created += 1

                        # Brief delay between partitions
                        await asyncio.sleep(5)

                    if partitions_created == 0:
                        logger.warning("No network partitions were created. Experiment may be misconfigured.")
                        self.emit_metric('chaos_experiment_status', 0, {'status': 'failed', 'reason': 'no_partitions_created'})
                        return False

                    # Monitor impact during partition
                    max_duration = max(
                        self._parse_duration(scenario.get('duration', '5m'))
                        for scenario in self.config['scenarios']
                    )

                    await self.monitor_partition_impact(max_duration)

                    # Cleanup partitions (auto-rollback)
                    cleanup_success = await self.cleanup_all_partitions()

                    if not cleanup_success:
                        logger.error("Failed to cleanup all network partitions")
                        self.emit_metric('chaos_experiment_status', 0, {'status': 'failed', 'reason': 'cleanup_failed'})
                        return False

                else:
                    logger.info("DRY RUN: Would have created network partitions but dry_run=true")
                    self.emit_metric('chaos_experiment_status', 1, {'status': 'dry_run'})

                # Wait for system to stabilize
                logger.info("Waiting for system stabilization...")
                await asyncio.sleep(30)

                # Verify system recovery
                post_experiment_healthy = True
                for check in self.config['safety']['health_checks']:
                    healthy, response_time = await self.perform_health_check(check)
                    if not healthy:
                        post_experiment_healthy = False
                        logger.warning(f"Service {check['name']} not healthy after experiment")

                if not post_experiment_healthy:
                    logger.error("System not fully healthy after experiment")
                    self.emit_metric('chaos_experiment_status', 0, {'status': 'failed', 'reason': 'post_experiment_unhealthy'})
                    return False

                # Mark experiment as successful
                experiment_duration = (datetime.utcnow() - self.start_time).total_seconds()
                self.emit_metric('chaos_experiment_duration_seconds', experiment_duration)
                self.emit_metric('chaos_experiment_status', 1, {'status': 'completed'})

                logger.info(f"Network partition experiment completed successfully in {experiment_duration:.2f} seconds")

                # Generate experiment report
                report = {
                    'experiment_id': self.experiment_id,
                    'start_time': self.start_time.isoformat(),
                    'duration_seconds': experiment_duration,
                    'baseline_metrics': baseline,
                    'partitions_created': self.active_partitions,
                    'status': 'success'
                }

                logger.info(f"Experiment report: {json.dumps(report, indent=2)}")
                return True

            except Exception as e:
                logger.error(f"Network partition experiment failed: {e}")
                self.emit_metric('chaos_experiment_status', 0, {'status': 'failed', 'reason': 'exception'})

                # Attempt cleanup on failure
                try:
                    await self.cleanup_all_partitions()
                except:
                    pass

                return False

            finally:
                await self.stop()

        def _parse_duration(self, duration_str):
            """Parse duration string like '5m' to seconds"""
            if duration_str.endswith('s'):
                return int(duration_str[:-1])
            elif duration_str.endswith('m'):
                return int(duration_str[:-1]) * 60
            elif duration_str.endswith('h'):
                return int(duration_str[:-1]) * 3600
            else:
                return int(duration_str)  # Assume seconds

    async def main():
        config_path = os.getenv('CONFIG_PATH', '/etc/config/config.yaml')
        experiment = NetworkPartitionChaosExperiment(config_path)

        success = await experiment.run_experiment()

        if not success:
            exit(1)

    if __name__ == "__main__":
        asyncio.run(main())

---
# Quarterly Network Partition Job
apiVersion: batch/v1
kind: Job
metadata:
  name: chaos-network-partition-quarterly
  namespace: chaos-testing
  labels:
    app: chaos-network-partition
    schedule: quarterly
    chaos-type: network-partition
spec:
  backoffLimit: 1
  template:
    metadata:
      labels:
        app: chaos-network-partition
        schedule: quarterly
        chaos-type: network-partition
      annotations:
        chaos.engineering/experiment: 'network-partition'
        chaos.engineering/schedule: 'quarterly'
    spec:
      serviceAccountName: chaos-engineering
      restartPolicy: OnFailure
      containers:
        - name: network-partition-chaos
          image: python:3.11-alpine
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -c
            - |
              pip install --no-cache-dir kubernetes pyyaml aiohttp requests
              exec python /scripts/network-partition.py
          env:
            - name: CONFIG_PATH
              value: '/etc/config/config.yaml'
            - name: CHAOS_EXPERIMENT_TYPE
              value: 'network-partition'
            - name: CHAOS_SCHEDULE
              value: 'quarterly'
          resources:
            requests:
              cpu: 200m
              memory: 512Mi
            limits:
              cpu: 1000m
              memory: 2Gi
          volumeMounts:
            - name: config
              mountPath: /etc/config
            - name: scripts
              mountPath: /scripts
      volumes:
        - name: config
          configMap:
            name: chaos-network-partition-config
        - name: scripts
          configMap:
            name: chaos-network-partition-config
            defaultMode: 0755
      nodeSelector:
        workload: chaos-testing

---
# Annual Comprehensive Network Partition Job
apiVersion: batch/v1
kind: Job
metadata:
  name: chaos-network-partition-annual
  namespace: chaos-testing
  labels:
    app: chaos-network-partition
    schedule: annual
    chaos-type: network-partition
spec:
  backoffLimit: 2
  template:
    metadata:
      labels:
        app: chaos-network-partition
        schedule: annual
        chaos-type: network-partition
      annotations:
        chaos.engineering/experiment: 'comprehensive-network-partition'
        chaos.engineering/schedule: 'annual'
    spec:
      serviceAccountName: chaos-engineering
      restartPolicy: OnFailure
      containers:
        - name: network-partition-chaos
          image: python:3.11-alpine
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -c
            - |
              pip install --no-cache-dir kubernetes pyyaml aiohttp requests

              # Create comprehensive annual test config
              cat > /tmp/annual-config.yaml << 'EOF'
              chaos:
                name: "comprehensive-network-partition"
                description: "Annual comprehensive network partition testing"
                experiment:
                  duration: "15m"
                  dry_run: false
                  safety_checks: true
                  auto_rollback: true
                scenarios:
                  - name: "full-database-isolation"
                    description: "Isolate entire database layer"
                    duration: "5m"
                    partition_type: "bidirectional"
                    source:
                      namespace: "pake-api"
                      selector:
                        app: "pake-api"
                    target:
                      namespace: "database"
                      selector:
                        tier: "database"
                  - name: "ai-service-isolation"
                    description: "Isolate AI services from all dependencies"
                    duration: "3m"
                    partition_type: "unidirectional"
                    direction: "to"
                    source:
                      namespace: "pake-ai"
                      selector:
                        app: "pake-ai-worker"
                    target:
                      namespace: "database"
                      selector: {}
                  - name: "cross-zone-latency"
                    description: "Simulate high cross-zone latency"
                    duration: "7m"
                    partition_type: "latency_injection"
                    latency: "500ms"
                    jitter: "100ms"
                    loss_rate: "5%"
                    source:
                      namespace: "pake-api"
                      selector:
                        app: "pake-api"
                    target:
                      namespace: "database"
                      selector:
                        role: "replica"
                safety:
                  protected_services:
                    - "kube-system/kube-dns"
                    - "kube-system/kube-apiserver"
                    - "monitoring/prometheus"
                    - "monitoring/grafana"
                  health_checks:
                    - name: "api-health"
                      url: "http://pake-api.pake-api.svc.cluster.local:8080/health"
                      timeout: 15
                      required: true
                    - name: "ai-health"
                      url: "http://pake-ai.pake-ai.svc.cluster.local:8080/health"
                      timeout: 15
                      required: true
                  success_criteria:
                    max_error_rate: 25.0
                    max_response_time: 10000
                    min_availability: 75.0
                monitoring:
                  metrics_interval: 10
                  trace_tags:
                    experiment: "comprehensive-network-partition"
                    environment: "production"
                    team: "platform"
                    schedule: "annual"
              EOF

              export CONFIG_PATH="/tmp/annual-config.yaml"
              exec python /scripts/network-partition.py
          env:
            - name: CHAOS_EXPERIMENT_TYPE
              value: 'network-partition-comprehensive'
            - name: CHAOS_SCHEDULE
              value: 'annual'
          resources:
            requests:
              cpu: 300m
              memory: 1Gi
            limits:
              cpu: 1500m
              memory: 4Gi
          volumeMounts:
            - name: config
              mountPath: /etc/config
            - name: scripts
              mountPath: /scripts
      volumes:
        - name: config
          configMap:
            name: chaos-network-partition-config
        - name: scripts
          configMap:
            name: chaos-network-partition-config
            defaultMode: 0755
